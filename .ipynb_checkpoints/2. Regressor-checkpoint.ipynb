{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_testing = pd.read_csv('../Tbrain_Insurance/testing-set.csv')\n",
    "#df_all = pd.read_csv('../Tbrain_Insurance/df_all.csv')\n",
    "df_all = pd.read_csv('../Tbrain_Insurance/df_policy_claim_unuique.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351273, 704)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nan_count = df_all.isnull().sum()\n",
    "# nan_count[nan_count > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# replace target: \n",
    "replace_date = {'09/2658','10/2622','02/2611','11/2602', '10/2582', '10/2581', '04/2579', '08/2569', '10/2521',\n",
    " '11/2512', '01/2512', '10/2472','08/2471', '11/2464', '03/2464', '12/2453', '01/2442', '07/2280', '02/2152'}\n",
    "print(len(replace_date))\n",
    "replace_date_set = set(replace_date)\n",
    "print(len(replace_date_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351273 entries, 0 to 351272\n",
      "Columns: 704 entries, Policy_Number to 2CSD3meanIO\n",
      "dtypes: float64(557), int64(141), object(6)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "## convert to datetime formate\n",
    "date_cols = ['DOB_of_Driver','Accident_Date','Accident_Time','ibirth', 'dbirth_new']\n",
    "\n",
    "\n",
    "for input1 in replace_date_set:\n",
    "    df_all[date_cols] = df_all[date_cols].replace(input1, pd.to_datetime('01/2017', format ='%m/%Y'))\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOB_of_Driver</th>\n",
       "      <th>Accident_Date</th>\n",
       "      <th>Accident_Time</th>\n",
       "      <th>ibirth</th>\n",
       "      <th>dbirth_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1980-11-01</td>\n",
       "      <td>1980-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1956-11-01</td>\n",
       "      <td>1982-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1954-03-01</td>\n",
       "      <td>1954-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1978-09-01</td>\n",
       "      <td>1978-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1960-10-01</td>\n",
       "      <td>1960-10-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DOB_of_Driver        Accident_Date        Accident_Time      ibirth  \\\n",
       "0  2017-01-01 00:00:00  2017-01-01 00:00:00  2017-01-01 00:00:00  1980-11-01   \n",
       "1  2017-01-01 00:00:00  2017-01-01 00:00:00  2017-01-01 00:00:00  1956-11-01   \n",
       "2  2017-01-01 00:00:00  2017-01-01 00:00:00  2017-01-01 00:00:00  1954-03-01   \n",
       "3  2017-01-01 00:00:00  2017-01-01 00:00:00  2017-01-01 00:00:00  1978-09-01   \n",
       "4  2017-01-01 00:00:00  2017-01-01 00:00:00  2017-01-01 00:00:00  1960-10-01   \n",
       "\n",
       "   dbirth_new  \n",
       "0  1980-11-01  \n",
       "1  1982-03-01  \n",
       "2  1954-03-01  \n",
       "3  1978-09-01  \n",
       "4  1960-10-01  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fill datetime value for date_cols \n",
    "df_all[date_cols] = df_all[date_cols].fillna(pd.to_datetime('01/2017', format ='%m/%Y'))\n",
    "df_all[date_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351273 entries, 0 to 351272\n",
      "Data columns (total 5 columns):\n",
      "DOB_of_Driver    351273 non-null datetime64[ns]\n",
      "Accident_Date    351273 non-null datetime64[ns]\n",
      "Accident_Time    351273 non-null datetime64[ns]\n",
      "ibirth           351273 non-null datetime64[ns]\n",
      "dbirth_new       351273 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](5)\n",
      "memory usage: 13.4 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351273 entries, 0 to 351272\n",
      "Columns: 704 entries, Policy_Number to 2CSD3meanIO\n",
      "dtypes: datetime64[ns](5), float64(557), int64(141), object(1)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "## convert to datetime formate\n",
    "date_cols = ['DOB_of_Driver','Accident_Date','Accident_Time','ibirth', 'dbirth_new']\n",
    "\n",
    "df = df_all\n",
    "\n",
    "date_col=['ibirth', 'dbirth_new']\n",
    "for col in date_col:\n",
    "    df[col] =  pd.to_datetime(df[col], format='%Y-%m-%d', errors='ignore')  #'%Y-%m-%d %H:%M:%S'   #errors='coerce'\n",
    "\n",
    "\n",
    "    \n",
    "date_col=['DOB_of_Driver', 'ibirth', 'dbirth_new']\n",
    "for col in date_col:\n",
    "    df[col] =  pd.to_datetime(df[col], format='%m/%Y', errors='ignore')  #'%Y-%m-%d %H:%M:%S'   #errors='coerce'\n",
    "\n",
    "date_col=['Accident_Date']\n",
    "for col in date_col:\n",
    "    df[col] =  pd.to_datetime(df[col], format='%Y/%m', errors='ignore')  #'%Y-%m-%d %H:%M:%S'   #errors='coerce'\n",
    "\n",
    "date_col=['Accident_Time']\n",
    "for col in date_col:\n",
    "    df[col] =  pd.to_datetime(df[col], format='%H:%M', errors='ignore')  #'%Y-%m-%d %H:%M:%S'   #errors='coerce'\n",
    "\n",
    "df[date_cols].info(verbose= True)\n",
    "df_all = df\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(date_cols, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152685742"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351273 entries, 0 to 351272\n",
      "Columns: 699 entries, Policy_Number to 2CSD3meanIO\n",
      "dtypes: float64(557), int64(141), object(1)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "df_all = df_all.fillna(0)\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nature & Juridical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nature\n",
    "df1 = df_all[(df_all['fsex_1']==1)|(df_all['fsex_2']==1)]\n",
    "\n",
    "df_train1 = df1[df1['Next_Premium']!=100]\n",
    "print(df_train1.shape)\n",
    "df_test1 = df1[df1['Next_Premium']==100]\n",
    "print(df_test1.shape)\n",
    "print(df_train1.shape[0]+df_test1.shape[0])\n",
    "print(df1.shape[0])\n",
    "\n",
    "# Juridical\n",
    "df2 = df_all[(df_all['fsex_ ']==1)]\n",
    "df2 = df2.drop(['i_age', 'd_age'], axis=1)    ######\n",
    "\n",
    "df_train2 = df2[df2['Next_Premium']!=100]\n",
    "print(df_train2.shape)\n",
    "df_test2 = df2[df2['Next_Premium']==100]\n",
    "print(df_test2.shape)\n",
    "print(df_train2.shape[0]+df_test2.shape[0])\n",
    "print(df2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Optional \n",
    "# # downsampling\n",
    "# df_train = df_train.sample(10000).reset_index()\n",
    "# df_train = df_train[df_train.columns[1:]]\n",
    "# df_train.head()\n",
    "# # df_test = df_test.sample (1400).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     4414\n",
       "1     8236\n",
       "2     6153\n",
       "3    12135\n",
       "4     4345\n",
       "Name: Next_Premium, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1=df_train1['Next_Premium']\n",
    "y1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34     2869\n",
       "44     1665\n",
       "47     3841\n",
       "65        0\n",
       "115    6383\n",
       "Name: Next_Premium, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2=df_train2['Next_Premium']\n",
    "y2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174910, 698)\n",
      "(174910,)\n",
      "(35853, 696)\n",
      "(35853,)\n"
     ]
    }
   ],
   "source": [
    "train_ID1 = df_train1['Policy_Number']\n",
    "df_train1 = df_train1.drop(['Policy_Number'], axis=1)\n",
    "print(df_train1.shape)\n",
    "print(train_ID1.shape)\n",
    "\n",
    "train_ID2 = df_train2['Policy_Number']\n",
    "df_train2 = df_train2.drop(['Policy_Number'], axis=1)\n",
    "print(df_train2.shape)\n",
    "print(train_ID2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120048, 698)\n",
      "(20462, 696)\n"
     ]
    }
   ],
   "source": [
    "df_test1=df_test1.drop(['Next_Premium'], axis=1)\n",
    "print(df_test1.shape)\n",
    "\n",
    "df_test2=df_test2.drop(['Next_Premium'], axis=1)\n",
    "print(df_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120048, 697)\n",
      "(120048,)\n",
      "(20462, 695)\n",
      "(20462,)\n"
     ]
    }
   ],
   "source": [
    "test_ID1=df_test1['Policy_Number']\n",
    "df_test1=df_test1.drop(['Policy_Number'], axis=1)\n",
    "print(df_test1.shape)\n",
    "print(test_ID1.shape)\n",
    "\n",
    "test_ID2=df_test2['Policy_Number']\n",
    "df_test2=df_test2.drop(['Policy_Number'], axis=1)\n",
    "print(df_test2.shape)\n",
    "print(test_ID2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1 = df_train1.drop(['Next_Premium'], axis=1)\n",
    "test1 = df_test1\n",
    "y_train1 = y1\n",
    "\n",
    "train2 = df_train2.drop(['Next_Premium'], axis=1)\n",
    "test2 = df_test2\n",
    "y_train2 = y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174910, 697)\n",
      "(120048, 697)\n",
      "(174910,)\n",
      "(35853, 695)\n",
      "(20462, 695)\n",
      "(35853,)\n"
     ]
    }
   ],
   "source": [
    "print(train1.shape)\n",
    "print(test1.shape)\n",
    "print(y1.shape)\n",
    "\n",
    "print(train2.shape)\n",
    "print(test2.shape)\n",
    "print(y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    " \n",
    "# ## standarlization\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# stdsc = StandardScaler()\n",
    "# train_std = stdsc.fit_transform(train)\n",
    "# test_std = stdsc.fit_transform(test)\n",
    " \n",
    "# X_std = train_std\n",
    "\n",
    "#     #求共變異係數矩陣\n",
    "# cov_mat = np.cov(X_std.T)\n",
    "# print(\"共變異係數矩陣.shape=\",cov_mat.shape)\n",
    "\n",
    " \n",
    "# #求共變異係數矩陣 的特徵向量及特徵值\n",
    "# eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "# print(\"特徵向量.shape=\",eigen_vecs.shape)\n",
    "\n",
    " \n",
    "# #計算解釋變異數比率 各特徵值/特徵值總和\n",
    "# tot = sum(eigen_vals)\n",
    "# var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "# cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "# # Make a list of (eigenvalue, eigenvector) tuples\n",
    "# eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n",
    "#                for i in range(len(eigen_vals))]\n",
    "\n",
    "# print(\"特徵值，特徵向量length：\",len(eigen_pairs))\n",
    "# eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n",
    "\n",
    "# #保留兩個最具影響力的特徵向量組成13x2 的投影矩陣W\n",
    "# w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "#                eigen_pairs[1][1][:, np.newaxis]))\n",
    "\n",
    "# train_pca = X_std.dot(w)\n",
    "# ###################################\n",
    "\n",
    "# X_std = test_std\n",
    "\n",
    "#     #求共變異係數矩陣\n",
    "# cov_mat = np.cov(X_std.T)\n",
    "# print(\"共變異係數矩陣.shape=\",cov_mat.shape)\n",
    "\n",
    " \n",
    "# #求共變異係數矩陣 的特徵向量及特徵值\n",
    "# eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "# print(\"特徵向量.shape=\",eigen_vecs.shape)\n",
    "\n",
    " \n",
    "# #計算解釋變異數比率 各特徵值/特徵值總和\n",
    "# tot = sum(eigen_vals)\n",
    "# var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "# cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "# # Make a list of (eigenvalue, eigenvector) tuples\n",
    "# eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n",
    "#                for i in range(len(eigen_vals))]\n",
    "\n",
    "# print(\"特徵值，特徵向量length：\",len(eigen_pairs))\n",
    "# eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n",
    "\n",
    "# #保留兩個最具影響力的特徵向量組成13x2 的投影矩陣W\n",
    "# w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "#                eigen_pairs[1][1][:, np.newaxis]))\n",
    "\n",
    "# test_pca = X_std.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = train_pca\n",
    "# test = test_pca\n",
    "# y_train = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def mae_cv1(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train1.values)\n",
    "    mae= (-cross_val_score(model, train1.values, y_train1.values, scoring=\"neg_mean_absolute_error\", cv = kf))\n",
    "    return(mae)\n",
    "\n",
    "def mae_cv2(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train2.values)\n",
    "    mae= (-cross_val_score(model, train2.values, y_train2.values, scoring=\"neg_mean_absolute_error\", cv = kf))\n",
    "    return(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # LASSO Regression :\n",
    "# # This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline :\n",
    "# lasso = make_pipeline(RobustScaler(), Lasso(alpha = 2, random_state=1))\n",
    "# print(\"lasso mae: \\n\", mae)\n",
    "# print(\"lasso score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Elastic Net Regression :\n",
    "# # again made robust to outliers:\n",
    "# #l1_list = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=2, l1_ratio=1, random_state=3))\n",
    "# mae = mae_cv(ENet)\n",
    "# print(\"ENet mae: \\n\", mae)\n",
    "# print(\"ENet score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Kernel Ridge Regression :\n",
    "# Kernel_list = ['linear','rbf','sigmoid','poly','laplacian','cosine','chi2']\n",
    "# alpha_list = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "\n",
    "# KRR = KernelRidge(alpha = 0.02, kernel = 'cosine', degree = 0) \n",
    "# # mae = mae_cv(KRR)\n",
    "# # print(\"KRR mae: \\n\", mae)\n",
    "# # print(\"KRR score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # With huber loss that makes it robust to outliers:\n",
    "# GBoost = GradientBoostingRegressor()\n",
    "# # mae = mae_cv(GBoost)\n",
    "# # print(\"GBoost mae: \\n\", mae)\n",
    "# # print(\"GBoost score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Averaged base models class\n",
    "# class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "#     def __init__(self, models):\n",
    "#         self.models = models\n",
    "        \n",
    "#     # we define clones of the original models to fit the data in\n",
    "#     def fit(self, X, y):\n",
    "#         self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "#         # Train cloned base models\n",
    "#         for model in self.models_:\n",
    "#             model.fit(X, y)\n",
    "\n",
    "#         return self\n",
    "    \n",
    "#     #Now we do the predictions for cloned models and average them\n",
    "#     def predict(self, X):\n",
    "#         predictions = np.column_stack([\n",
    "#             model.predict(X) for model in self.models_\n",
    "#         ])\n",
    "#         return np.mean(predictions, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## mae Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae = mae_cv(lasso)\n",
    "# mae_lasso = mae \n",
    "# print(\"lasso mae: \\n\", mae)\n",
    "# print(\"lasso score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lasso mae: \n",
    "#  [ 1650.25138092  2758.60603395  2507.70387202  2472.04363955  2150.09175628]\n",
    "# lasso score: 2307.7393 (381.4199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae = mae_cv(ENet)\n",
    "# mae_ENet = mae\n",
    "# print(\"ENet mae: \\n\", mae)\n",
    "# print(\"ENet score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ENet mae: \n",
    "#  [ 1647.98492773  2781.01160241  2516.92456053  2486.03839108  2164.61988768]\n",
    "# ENet score: 2319.3159 (388.4906)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae = (mae_cv(KRR))\n",
    "# mae_KRR = mae\n",
    "# print(\"KRR mae: \\n\", mae)\n",
    "# print(\"KRR score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae = mae_cv(GBoost)\n",
    "# mae_GBoost = mae\n",
    "# print(\"GBoost mae: \\n\", mae)\n",
    "# print(\"GBoost score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GBoost mae: \n",
    "#  [ 1461.50882094  2446.82567999  2218.31836897  2210.17782972  1910.665789  ]\n",
    "# GBoost score: 2049.4993 (339.7626)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Averaged base models score\n",
    "# averaged_models = AveragingModels(models = (lasso, ENet, GBoost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae = mae_cv(averaged_models)\n",
    "# mae_Avg = mae\n",
    "# print(\" Averaged base models: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Averaged base models: 2189.5514 (372.8555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Meta-Model\n",
    "# #Stacking averaged Models Class:\n",
    "# class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "#     def __init__(self, base_models, meta_model, n_folds=5):\n",
    "#         self.base_models = base_models\n",
    "#         self.meta_model = meta_model\n",
    "#         self.n_folds = n_folds\n",
    "   \n",
    "#     # We again fit the data on clones of the original models\n",
    "#     def fit(self, X, y):\n",
    "#         self.base_models_ = [list() for x in self.base_models]\n",
    "#         self.meta_model_ = clone(self.meta_model)\n",
    "#         kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "#         # Train cloned base models then create out-of-fold predictions\n",
    "#         # that are needed to train the cloned meta-model\n",
    "#         out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "#         for i, model in enumerate(self.base_models):\n",
    "#             for train_index, holdout_index in kfold.split(X, y):\n",
    "#                 instance = clone(model)\n",
    "#                 self.base_models_[i].append(instance)\n",
    "#                 instance.fit(X[train_index], y[train_index])\n",
    "#                 y_pred = instance.predict(X[holdout_index])\n",
    "#                 out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "#         # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "#         self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "#         return self\n",
    "   \n",
    "#     #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "#     #meta-features for the final prediction which is done by the meta-model\n",
    "#     def predict(self, X):\n",
    "#         meta_features = np.column_stack([\n",
    "#             np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "#             for base_models in self.base_models_ ])\n",
    "#         return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mae_1(y1, y_pred1):\n",
    "    return mean_absolute_error(y1,y_pred1)\n",
    "def mae_2(y2, y_pred2):\n",
    "    return mean_absolute_error(y2,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Stacking Averaged models Score\n",
    "\n",
    "# stacked_averaged_models = StackingAveragedModels(base_models = (ENet, lasso, GBoost),  #ENet, lasso, KRR\n",
    "#                                                  meta_model = GBoost) #GBoost\n",
    "\n",
    "# mae = mae_cv(stacked_averaged_models)\n",
    "# mae_stacked_avg = mae\n",
    "# print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stacking Averaged models score: 2042.9411 (339.1027)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# more models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# lgb mae: \n",
    "#  [ 1501.23043208  2595.38487591  2334.24177316  2318.40761041  1971.9493555 ]\n",
    "# lgb score: 2144.2428 (377.6538)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgb mae1: \n",
      " [ 1434.84210925  2260.83808605  2150.82386376  2179.23853216  1866.96877933]\n",
      "lgb score1: 1978.5423 (302.5589)\n",
      "\n",
      "lgb mae2: \n",
      " [ 3867.42438097  4014.83936575  3732.4610646   3878.76570465  3291.6854473 ]\n",
      "lgb score1: 3757.0352 (249.2474)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LGBM :\n",
    "model_lgb1 = lgb.LGBMRegressor(objective='regression')\n",
    "mae1 = mae_cv1(model_lgb1)\n",
    "mae_lgb1 = mae1\n",
    "print(\"lgb mae1: \\n\", mae1)\n",
    "print(\"lgb score1: {:.4f} ({:.4f})\\n\".format(mae1.mean(), mae1.std()))\n",
    "\n",
    "model_lgb2 = lgb.LGBMRegressor(objective='regression')\n",
    "mae2 = mae_cv2(model_lgb2)\n",
    "mae_lgb2 = mae2\n",
    "print(\"lgb mae2: \\n\", mae2)\n",
    "print(\"lgb score1: {:.4f} ({:.4f})\\n\".format(mae2.mean(), mae2.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae_lgb on train1: 1969.9914609\n",
      "Feature ranking:\n",
      "1. feature 112.: Premiumtr (64800.000000)\n",
      "2. feature 7.: pdmg_acctr (28300.000000)\n",
      "3. feature 22.: Premium_SumBtr (20200.000000)\n",
      "4. feature 190.: I_P_ratio3tr (18000.000000)\n",
      "5. feature 19.: cont_ratiotr (17900.000000)\n",
      "6. feature 186.: Insured_Amount3tr (15200.000000)\n",
      "7. feature 188.: I_P_ratio1tr (10000.000000)\n",
      "mae_lgb on train2: 3718.18953465\n",
      "Feature ranking:\n",
      "1. feature 110.: Premiumtr (73200.000000)\n",
      "2. feature 19.: cont_ratiotr (24500.000000)\n",
      "3. feature 71.: fmarriage_ tr (16400.000000)\n",
      "4. feature 184.: Insured_Amount3tr (16300.000000)\n",
      "5. feature 75.: Insured_Amount1_BSumtr (10600.000000)\n",
      "6. feature 190.: P_R_pctAtr (9600.000000)\n",
      "7. feature 81.: Insured_Amount3_BSumtr (8700.000000)\n"
     ]
    }
   ],
   "source": [
    "model = model_lgb1  #\n",
    "\n",
    "model.fit(train1, y_train1)\n",
    "train_pred_lgb1 = model.predict(train1)\n",
    "mae1 = mae_1(y_train1, train_pred_lgb1)  #\n",
    "mae_lgb_train1 = mae1                                   #\n",
    "print('mae_lgb on train1:', mae1) #\n",
    "\n",
    "y_pred_lgb1 = model.predict(test1)  #\n",
    "\n",
    "importances1 = model.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "indices1 = np.argsort(importances1)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(7):   # train.shape[1]\n",
    "    print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices1[f], train1.columns[indices1[f]], importances1[indices1[f]]*100))\n",
    "\n",
    "# # Plot the feature importances of the forest\n",
    "# plt.figure()\n",
    "# plt.title(\"Feature importances\")\n",
    "# plt.bar(range(train.shape[1]), importances[indices],\n",
    "#        color=\"r\") #, yerr=std[indices], align=\"center\")\n",
    "# plt.xticks(range(train.shape[1]), indices)\n",
    "# plt.xlim([-1, train.shape[1]])\n",
    "# plt.show()\n",
    "\n",
    "model = model_lgb2  #\n",
    "model.fit(train2, y_train2)\n",
    "train_pred_lgb2 = model.predict(train2)\n",
    "mae2 = mae_2(y_train2, train_pred_lgb2)  #\n",
    "mae_lgb_train2 = mae2                                   #\n",
    "print('mae_lgb on train2:', mae2) #\n",
    "\n",
    "y_pred_lgb2 = model.predict(test2)  #\n",
    "\n",
    "importances2 = model.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "indices2 = np.argsort(importances2)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(7):   # train.shape[1]\n",
    "    print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices2[f], train2.columns[indices2[f]], importances2[indices2[f]]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# Xgboost mae: \n",
    "#  [ 1553.09530469  2401.15759272  2177.35396934  2174.6915555   1881.35674227]\n",
    "# Xgboost score: 2037.5310 (293.1403)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost mae1: \n",
      " [ 1350.37133052  2014.22129356  1960.73757804  1954.55850098  1718.77444599]\n",
      "Xgboost score1: 1799.7326 (246.7302)\n",
      "\n",
      "Xgboost mae2: \n",
      " [ 3225.88786955  3281.65027565  3100.98577126  3315.29838637  2818.6427096 ]\n",
      "Xgboost score2: 3148.4930 (180.3347)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost :\n",
    "model_xgb1 = xgb.XGBRegressor() #n_estimators=200\n",
    "mae1 = mae_cv1(model_xgb1)\n",
    "mae_xgb1 = mae2\n",
    "print(\"Xgboost mae1: \\n\", mae1)\n",
    "print(\"Xgboost score1: {:.4f} ({:.4f})\\n\".format(mae1.mean(), mae1.std()))\n",
    "\n",
    "model_xgb2 = xgb.XGBRegressor() #n_estimators=200\n",
    "mae2 = mae_cv2(model_xgb2)\n",
    "mae_xgb2 = mae2\n",
    "print(\"Xgboost mae2: \\n\", mae2)\n",
    "print(\"Xgboost score2: {:.4f} ({:.4f})\\n\".format(mae2.mean(), mae2.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Xgboost mae: \n",
    "#  [ 2209.30725367  2330.14260246  2082.15111059  2190.86471294  2111.35915974]\n",
    "# Xgboost score: 2184.7650 (86.8242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae_xgb on train: 1762.2161095\n",
      "Feature ranking:\n",
      "1. feature 112.: Premiumtr (22.740965)\n",
      "2. feature 20.: Premium_GroupSumtr (5.873494)\n",
      "3. feature 212.: Premium_stdtr (4.518072)\n",
      "4. feature 83.: Insured_Amount3_BSumtr (4.367470)\n",
      "5. feature 19.: cont_ratiotr (3.614458)\n",
      "6. feature 7.: pdmg_acctr (3.012048)\n",
      "7. feature 14.: Policy_Number_Counttr (2.710843)\n",
      "mae_xgb on train: 2970.57916256\n",
      "Feature ranking:\n",
      "1. feature 110.: Premiumtr (16.741405)\n",
      "2. feature 19.: cont_ratiotr (4.035874)\n",
      "3. feature 1.: Engine_Displacement_(Cubic_Centimeter)tr (3.886398)\n",
      "4. feature 81.: Insured_Amount3_BSumtr (3.886398)\n",
      "5. feature 181.: Premium_meantr (3.736921)\n",
      "6. feature 20.: Premium_GroupSumtr (3.288490)\n",
      "7. feature 75.: Insured_Amount1_BSumtr (3.139013)\n"
     ]
    }
   ],
   "source": [
    "model = model_xgb1  #\n",
    "\n",
    "model.fit(train1, y_train1)\n",
    "train_pred_xgb1 = model.predict(train1)  #\n",
    "mae1 = mae_1(y_train1, train_pred_xgb1)   #\n",
    "mae_xgb_train1 = mae1                                   #\n",
    "print('mae_xgb on train:', mae1) #\n",
    "\n",
    "y_pred_xgb1 = model.predict(test1)  #\n",
    "\n",
    "importances1 = model.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "indices1 = np.argsort(importances1)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(7):   # train.shape[1]\n",
    "    print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices1[f], train1.columns[indices1[f]], importances1[indices1[f]]*100))\n",
    "\n",
    "\n",
    "model = model_xgb2  #    \n",
    "model.fit(train2, y_train2)\n",
    "train_pred_xgb2 = model.predict(train2)  #\n",
    "mae2 = mae_2(y_train2, train_pred_xgb2)   #\n",
    "mae_xgb_train2 = mae2                                   #\n",
    "print('mae_xgb on train:', mae2) #\n",
    "\n",
    "y_pred_xgb2 = model.predict(test2)  #\n",
    "\n",
    "importances2 = model.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "indices2 = np.argsort(importances2)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(7):   # train.shape[1]\n",
    "    print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices2[f], train2.columns[indices2[f]], importances2[indices2[f]]*100))\n",
    "\n",
    "# # Plot the feature importances of the forest\n",
    "# plt.figure()\n",
    "# plt.title(\"Feature importances\")\n",
    "# plt.bar(range(train.shape[1]), importances[indices],\n",
    "#        color=\"r\") #, yerr=std[indices], align=\"center\")\n",
    "# plt.xticks(range(train.shape[1]), indices)\n",
    "# plt.xlim([-1, train.shape[1]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae_xgb on train: 1924.18772008\n",
    "# Feature ranking:\n",
    "# 1. feature 166.: Premiumtr (19.466248)\n",
    "# 2. feature 250.: Premium_meantr (8.163265)\n",
    "# 3. feature 258.: Premium_Pred_Addtr (5.808477)\n",
    "# 4. feature 72.: Replacement_cost_of_insured_vehicletr (4.709576)\n",
    "# 5. feature 98.: Insured_Amount1tr (4.395605)\n",
    "# 6. feature 71.: Engine_Displacement_(Cubic_Centimeter)tr (3.767661)\n",
    "# 7. feature 165.: Coverage_Deductibletr (3.453689)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# RandomForest mae: \n",
    "#  [ 1550.90146598  2462.00879027  2237.7326891   2258.35016851  1942.53006033]\n",
    "\n",
    "# RandomForest score: 2090.3046 (316.5222)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # RF :\n",
    "# model_rf = RandomForestRegressor(n_estimators=100)  # default n_estimators = 10\n",
    "# mae = mae_cv(model_rf)\n",
    "# mae_rf = mae\n",
    "# print(\"RandomForest mae: \\n\", mae)\n",
    "# print(\"\\nRandomForest score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RandomForest mae: \n",
    "#  [ 1487.6104638   2423.44725241  2203.43303404  2212.03221765  1904.53282157]\n",
    "\n",
    "# RandomForest score: 2046.2112 (324.5396)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = model_rf  #\n",
    "\n",
    "# model.fit(train, y_train)\n",
    "# train_pred_rf = model.predict(train)  #\n",
    "# mae = mae_(y_train, train_pred_rf)   #\n",
    "# mae_rf_train = mae                                   #\n",
    "# print('mae_rf on train:', mae) #\n",
    "\n",
    "# y_pred_rf = model.predict(test)  #\n",
    "\n",
    "# importances = model.feature_importances_\n",
    "# #std = np.std([tree.feature_importances_ for tree in model_rf.estimators_], axis=0)\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# # Print the feature ranking\n",
    "# print(\"Feature ranking:\")\n",
    "\n",
    "# for f in range(7):\n",
    "#     print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices[f], train.columns[indices[f]], importances[indices[f]]*100))\n",
    "\n",
    "# # # Plot the feature importances of the forest\n",
    "# # plt.figure()\n",
    "# # plt.title(\"Feature importances\")\n",
    "# # plt.bar(range(train.shape[1]), importances[indices],\n",
    "# #        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "# # plt.xticks(range(train.shape[1]), indices)\n",
    "# # plt.xlim([-1, train.shape[1]])\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae_rf on train: 801.865598\n",
    "# Feature ranking:\n",
    "# 1. feature 166.: Premiumtr (29.192158)\n",
    "# 2. feature 167.: Premium_Predtr (26.174226)\n",
    "# 3. feature 258.: Premium_Pred_Addtr (8.405166)\n",
    "# 4. feature 72.: Replacement_cost_of_insured_vehicletr (5.817675)\n",
    "# 5. feature 71.: Engine_Displacement_(Cubic_Centimeter)tr (1.952259)\n",
    "# 6. feature 242.: Insured_Amount1_meantr (1.222075)\n",
    "# 7. feature 78.: aassured_ziptr (1.193084)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Final Training and Prediction: \n",
    "# stacked_averaged_models.fit(train.values, y_train.values)\n",
    "# stacked_train_pred = stacked_averaged_models.predict(train.values)\n",
    "# stacked_pred = stacked_averaged_models.predict(test.values)\n",
    "\n",
    "# print(mae_(y_train, stacked_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = 0.3\n",
    "x = 0.7\n",
    "# r = 0.11\n",
    "# s = 1- l - x - r\n",
    "# ensemble_train = stacked_train_pred * s + train_pred_lgb *l + train_pred_xgb* x + train_pred_rf * r\n",
    "# ensemble_test = stacked_pred * s + y_pred_lgb * l + y_pred_xgb * x + y_pred_rf *r\n",
    "\n",
    "ensemble_train1 =  train_pred_lgb1 *l + train_pred_xgb1* x \n",
    "ensemble_test1 =  y_pred_lgb1 * l + y_pred_xgb1 * x \n",
    "\n",
    "ensemble_train2 =  train_pred_lgb2 *l + train_pred_xgb2* x \n",
    "ensemble_test2 =  y_pred_lgb2 * l + y_pred_xgb2 * x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE score on train data1:\n",
      "1814.63212949\n",
      "MAE score on train data2:\n",
      "3148.03033507\n"
     ]
    }
   ],
   "source": [
    "print('MAE score on train data1:')  # 1844.5\n",
    "print(mae_1(y_train1, ensemble_train1))\n",
    "\n",
    "print('MAE score on train data2:')  # 1844.5\n",
    "print(mae_2(y_train2, ensemble_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## prep submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final</th>\n",
       "      <th>lgb</th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76305</th>\n",
       "      <td>2195.615900</td>\n",
       "      <td>2195.615900</td>\n",
       "      <td>2236.253174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107661</th>\n",
       "      <td>5133.944899</td>\n",
       "      <td>5133.944899</td>\n",
       "      <td>5332.768066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116059</th>\n",
       "      <td>3071.113329</td>\n",
       "      <td>3071.113329</td>\n",
       "      <td>3180.338623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34040</th>\n",
       "      <td>3721.948530</td>\n",
       "      <td>3721.948530</td>\n",
       "      <td>4262.590820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85071</th>\n",
       "      <td>4102.560449</td>\n",
       "      <td>4102.560449</td>\n",
       "      <td>4308.459473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3744</th>\n",
       "      <td>2195.615900</td>\n",
       "      <td>2195.615900</td>\n",
       "      <td>2202.098389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35252</th>\n",
       "      <td>2462.276028</td>\n",
       "      <td>2462.276028</td>\n",
       "      <td>2494.897217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109905</th>\n",
       "      <td>1367.472970</td>\n",
       "      <td>1367.472970</td>\n",
       "      <td>1245.308594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43252</th>\n",
       "      <td>3329.882883</td>\n",
       "      <td>3329.882883</td>\n",
       "      <td>3527.113281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95809</th>\n",
       "      <td>1952.435399</td>\n",
       "      <td>1952.435399</td>\n",
       "      <td>2022.292236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Final          lgb          xgb\n",
       "76305   2195.615900  2195.615900  2236.253174\n",
       "107661  5133.944899  5133.944899  5332.768066\n",
       "116059  3071.113329  3071.113329  3180.338623\n",
       "34040   3721.948530  3721.948530  4262.590820\n",
       "85071   4102.560449  4102.560449  4308.459473\n",
       "3744    2195.615900  2195.615900  2202.098389\n",
       "35252   2462.276028  2462.276028  2494.897217\n",
       "109905  1367.472970  1367.472970  1245.308594\n",
       "43252   3329.882883  3329.882883  3527.113281\n",
       "95809   1952.435399  1952.435399  2022.292236"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_pred_stacked = pd.DataFrame(stacked_pred)\n",
    "y_pred_lgb1 = pd.DataFrame(y_pred_lgb1)\n",
    "y_pred_xgb1 = pd.DataFrame(y_pred_xgb1)\n",
    "# y_pred_rf = pd.DataFrame(y_pred_rf)\n",
    "\n",
    "y_pred1 = pd.DataFrame(ensemble_test1)\n",
    "\n",
    "all_y1 = pd.concat([y_pred1,  y_pred_lgb1, y_pred_xgb1], axis=1)\n",
    "all_y1.columns=['Final', 'lgb','xgb']\n",
    "all_y1.to_csv('../Tbrain_Insurance/Reg_y1.csv')\n",
    "all_y1.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final</th>\n",
       "      <th>lgb</th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12768</th>\n",
       "      <td>4099.559191</td>\n",
       "      <td>4099.559191</td>\n",
       "      <td>4230.724121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>2914.974019</td>\n",
       "      <td>2914.974019</td>\n",
       "      <td>2699.940186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13271</th>\n",
       "      <td>4602.813560</td>\n",
       "      <td>4602.813560</td>\n",
       "      <td>4626.244141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19480</th>\n",
       "      <td>3735.384120</td>\n",
       "      <td>3735.384120</td>\n",
       "      <td>3888.444336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12956</th>\n",
       "      <td>15436.043162</td>\n",
       "      <td>15436.043162</td>\n",
       "      <td>18112.279297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>2911.040206</td>\n",
       "      <td>2911.040206</td>\n",
       "      <td>2579.435547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17832</th>\n",
       "      <td>4960.765620</td>\n",
       "      <td>4960.765620</td>\n",
       "      <td>4927.326660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8745</th>\n",
       "      <td>4191.044624</td>\n",
       "      <td>4191.044624</td>\n",
       "      <td>4230.724121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15669</th>\n",
       "      <td>5031.066568</td>\n",
       "      <td>5031.066568</td>\n",
       "      <td>4909.228027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14715</th>\n",
       "      <td>10614.073316</td>\n",
       "      <td>10614.073316</td>\n",
       "      <td>11172.598633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Final           lgb           xgb\n",
       "12768   4099.559191   4099.559191   4230.724121\n",
       "2811    2914.974019   2914.974019   2699.940186\n",
       "13271   4602.813560   4602.813560   4626.244141\n",
       "19480   3735.384120   3735.384120   3888.444336\n",
       "12956  15436.043162  15436.043162  18112.279297\n",
       "262     2911.040206   2911.040206   2579.435547\n",
       "17832   4960.765620   4960.765620   4927.326660\n",
       "8745    4191.044624   4191.044624   4230.724121\n",
       "15669   5031.066568   5031.066568   4909.228027\n",
       "14715  10614.073316  10614.073316  11172.598633"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_pred_stacked = pd.DataFrame(stacked_pred)\n",
    "y_pred_lgb2 = pd.DataFrame(y_pred_lgb2)\n",
    "y_pred_xgb2 = pd.DataFrame(y_pred_xgb2)\n",
    "# y_pred_rf = pd.DataFrame(y_pred_rf)\n",
    "\n",
    "y_pred2 = pd.DataFrame(ensemble_test2)\n",
    "\n",
    "all_y2 = pd.concat([y_pred2,  y_pred_lgb2, y_pred_xgb2], axis=1)\n",
    "all_y2.columns=['Final', 'lgb','xgb']\n",
    "all_y2.to_csv('../Tbrain_Insurance/Reg_y2.csv')\n",
    "all_y2.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_y1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_y2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative prediction1 (#):  0\n",
      "negative prediction1 (Percent): 0.000000\n",
      "negative prediction2 (#):  0\n",
      "negative prediction2 (Percent): 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Negative -> 0 \n",
    "b = all_y1[all_y1['Final']<0].shape[0]\n",
    "a = b/all_y1.shape[0]*100\n",
    "print((\"negative prediction1 (#): %2d\"%b))\n",
    "print(\"negative prediction1 (Percent): %2f\"%a)\n",
    "\n",
    "b = all_y2[all_y2['Final']<0].shape[0]\n",
    "a = b/all_y2.shape[0]*100\n",
    "print((\"negative prediction2 (#): %2d\"%b))\n",
    "print(\"negative prediction2 (Percent): %2f\"%a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# condition1 = all_y['Final']<0\n",
    "# all_y['Final'].loc[condition1] = 0\n",
    "# all_y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140510, 2)\n",
      "(120048,)\n",
      "(120048, 1)\n",
      "(20462,)\n",
      "(20462, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df_testing.shape)\n",
    "print(test_ID1.shape)\n",
    "print(y_pred1.shape)\n",
    "print(test_ID2.shape)\n",
    "print(y_pred2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Policy_Number</th>\n",
       "      <th>Next_Premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55789b8f86893761c9aa9e7bf17938e737decc68</td>\n",
       "      <td>3024.166521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6df13a3384528ba6339c52b4fff7c149de68011</td>\n",
       "      <td>1723.786276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e112d926103147bcdcb6dab201b736185a3e2520</td>\n",
       "      <td>2299.211204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa346fa4b1931d1c7a55f8e1bca40b0927dd65ac</td>\n",
       "      <td>6250.484252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39c4d5daaa791676ec5559c9066d7e8e8dfc51d7</td>\n",
       "      <td>2603.697796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Policy_Number  Next_Premium\n",
       "0  55789b8f86893761c9aa9e7bf17938e737decc68   3024.166521\n",
       "1  b6df13a3384528ba6339c52b4fff7c149de68011   1723.786276\n",
       "2  e112d926103147bcdcb6dab201b736185a3e2520   2299.211204\n",
       "3  aa346fa4b1931d1c7a55f8e1bca40b0927dd65ac   6250.484252\n",
       "4  39c4d5daaa791676ec5559c9066d7e8e8dfc51d7   2603.697796"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(test_ID1).reset_index()\n",
    "# df1['Next_Premium'] = y_pred_lgb1\n",
    "df1['Next_Premium'] = all_y1['Final']  ##\n",
    "\n",
    "df2 = pd.DataFrame(test_ID2).reset_index()\n",
    "# df2['Next_Premium'] = y_pred_lgb2\n",
    "df2['Next_Premium'] = all_y2['Final'] ##\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df = df_testing.merge(df, how = 'left', on = 'Policy_Number', suffixes=('_x','_y'))\n",
    "DF = df.drop(['index','Next_Premium_x'], axis = 1)\n",
    "DF.columns = df_testing.columns\n",
    "DF.to_csv('../Tbrain_Insurance/testing-set_en.csv',index = False)     ##\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Policy_Number</th>\n",
       "      <th>Next_Premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55789b8f86893761c9aa9e7bf17938e737decc68</td>\n",
       "      <td>3024.166521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6df13a3384528ba6339c52b4fff7c149de68011</td>\n",
       "      <td>1723.786276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e112d926103147bcdcb6dab201b736185a3e2520</td>\n",
       "      <td>2299.211204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa346fa4b1931d1c7a55f8e1bca40b0927dd65ac</td>\n",
       "      <td>6250.484252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39c4d5daaa791676ec5559c9066d7e8e8dfc51d7</td>\n",
       "      <td>2603.697796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Policy_Number  Next_Premium\n",
       "0  55789b8f86893761c9aa9e7bf17938e737decc68   3024.166521\n",
       "1  b6df13a3384528ba6339c52b4fff7c149de68011   1723.786276\n",
       "2  e112d926103147bcdcb6dab201b736185a3e2520   2299.211204\n",
       "3  aa346fa4b1931d1c7a55f8e1bca40b0927dd65ac   6250.484252\n",
       "4  39c4d5daaa791676ec5559c9066d7e8e8dfc51d7   2603.697796"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(test_ID1).reset_index()\n",
    "# df1['Next_Premium'] = y_pred_lgb1\n",
    "df1['Next_Premium'] = all_y1['lgb']  ##\n",
    "\n",
    "df2 = pd.DataFrame(test_ID2).reset_index()\n",
    "# df2['Next_Premium'] = y_pred_lgb2\n",
    "df2['Next_Premium'] = all_y2['lgb'] ##\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df.head()\n",
    "\n",
    "df = df_testing.merge(df, how = 'left', on = 'Policy_Number', suffixes=('_x','_y'))\n",
    "DF = df.drop(['index','Next_Premium_x'], axis = 1)\n",
    "DF.columns = df_testing.columns\n",
    "DF.to_csv('../Tbrain_Insurance/testing-set_lgb.csv',index = False)     ##\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Policy_Number</th>\n",
       "      <th>Next_Premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55789b8f86893761c9aa9e7bf17938e737decc68</td>\n",
       "      <td>3058.740967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6df13a3384528ba6339c52b4fff7c149de68011</td>\n",
       "      <td>1551.356567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e112d926103147bcdcb6dab201b736185a3e2520</td>\n",
       "      <td>1649.002808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa346fa4b1931d1c7a55f8e1bca40b0927dd65ac</td>\n",
       "      <td>6763.020020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39c4d5daaa791676ec5559c9066d7e8e8dfc51d7</td>\n",
       "      <td>2588.228760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Policy_Number  Next_Premium\n",
       "0  55789b8f86893761c9aa9e7bf17938e737decc68   3058.740967\n",
       "1  b6df13a3384528ba6339c52b4fff7c149de68011   1551.356567\n",
       "2  e112d926103147bcdcb6dab201b736185a3e2520   1649.002808\n",
       "3  aa346fa4b1931d1c7a55f8e1bca40b0927dd65ac   6763.020020\n",
       "4  39c4d5daaa791676ec5559c9066d7e8e8dfc51d7   2588.228760"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(test_ID1).reset_index()\n",
    "# df1['Next_Premium'] = y_pred_lgb1\n",
    "df1['Next_Premium'] = all_y1['xgb']  ##\n",
    "\n",
    "df2 = pd.DataFrame(test_ID2).reset_index()\n",
    "# df2['Next_Premium'] = y_pred_lgb2\n",
    "df2['Next_Premium'] = all_y2['xgb'] ##\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df = df_testing.merge(df, how = 'left', on = 'Policy_Number', suffixes=('_x','_y'))\n",
    "DF = df.drop(['index','Next_Premium_x'], axis = 1)\n",
    "DF.columns = df_testing.columns\n",
    "DF.to_csv('../Tbrain_Insurance/testing-set_xgb.csv',index = False)     ##\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
