{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_testing = pd.read_csv('../Tbrain_Insurance/testing-set.csv')\n",
    "#df_all = pd.read_csv('../Tbrain_Insurance/df_all.csv')\n",
    "df_all = pd.read_csv('../Tbrain_Insurance/df_policy_claim_unuique.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351273, 704)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nan_count = df_all.isnull().sum()\n",
    "# nan_count[nan_count > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# replace target: \n",
    "replace_date = {'09/2658','10/2622','02/2611','11/2602', '10/2582', '10/2581', '04/2579', '08/2569', '10/2521',\n",
    " '11/2512', '01/2512', '10/2472','08/2471', '11/2464', '03/2464', '12/2453', '01/2442', '07/2280', '02/2152'}\n",
    "print(len(replace_date))\n",
    "replace_date_set = set(replace_date)\n",
    "print(len(replace_date_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351273 entries, 0 to 351272\n",
      "Columns: 704 entries, Policy_Number to 2CSD3meanIO\n",
      "dtypes: float64(557), int64(141), object(6)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "## convert to datetime formate\n",
    "date_cols = ['DOB_of_Driver','Accident_Date','Accident_Time','ibirth', 'dbirth_new']\n",
    "\n",
    "\n",
    "for input1 in replace_date_set:\n",
    "    df_all[date_cols] = df_all[date_cols].replace(input1, pd.to_datetime('01/2017', format ='%m/%Y'))\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOB_of_Driver</th>\n",
       "      <th>Accident_Date</th>\n",
       "      <th>Accident_Time</th>\n",
       "      <th>ibirth</th>\n",
       "      <th>dbirth_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1980-11-01</td>\n",
       "      <td>1980-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1956-11-01</td>\n",
       "      <td>1982-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1954-03-01</td>\n",
       "      <td>1954-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1978-09-01</td>\n",
       "      <td>1978-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1960-10-01</td>\n",
       "      <td>1960-10-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DOB_of_Driver        Accident_Date        Accident_Time      ibirth  \\\n",
       "0  2017-01-01 00:00:00  2017-01-01 00:00:00  2017-01-01 00:00:00  1980-11-01   \n",
       "1  2017-01-01 00:00:00  2017-01-01 00:00:00  2017-01-01 00:00:00  1956-11-01   \n",
       "2  2017-01-01 00:00:00  2017-01-01 00:00:00  2017-01-01 00:00:00  1954-03-01   \n",
       "3  2017-01-01 00:00:00  2017-01-01 00:00:00  2017-01-01 00:00:00  1978-09-01   \n",
       "4  2017-01-01 00:00:00  2017-01-01 00:00:00  2017-01-01 00:00:00  1960-10-01   \n",
       "\n",
       "   dbirth_new  \n",
       "0  1980-11-01  \n",
       "1  1982-03-01  \n",
       "2  1954-03-01  \n",
       "3  1978-09-01  \n",
       "4  1960-10-01  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fill datetime value for date_cols \n",
    "df_all[date_cols] = df_all[date_cols].fillna(pd.to_datetime('01/2017', format ='%m/%Y'))\n",
    "df_all[date_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351273 entries, 0 to 351272\n",
      "Data columns (total 5 columns):\n",
      "DOB_of_Driver    351273 non-null datetime64[ns]\n",
      "Accident_Date    351273 non-null datetime64[ns]\n",
      "Accident_Time    351273 non-null datetime64[ns]\n",
      "ibirth           351273 non-null datetime64[ns]\n",
      "dbirth_new       351273 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](5)\n",
      "memory usage: 13.4 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351273 entries, 0 to 351272\n",
      "Columns: 704 entries, Policy_Number to 2CSD3meanIO\n",
      "dtypes: datetime64[ns](5), float64(557), int64(141), object(1)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "## convert to datetime formate\n",
    "date_cols = ['DOB_of_Driver','Accident_Date','Accident_Time','ibirth', 'dbirth_new']\n",
    "\n",
    "df = df_all\n",
    "\n",
    "date_col=['ibirth', 'dbirth_new']\n",
    "for col in date_col:\n",
    "    df[col] =  pd.to_datetime(df[col], format='%Y-%m-%d', errors='ignore')  #'%Y-%m-%d %H:%M:%S'   #errors='coerce'\n",
    "\n",
    "\n",
    "    \n",
    "date_col=['DOB_of_Driver', 'ibirth', 'dbirth_new']\n",
    "for col in date_col:\n",
    "    df[col] =  pd.to_datetime(df[col], format='%m/%Y', errors='ignore')  #'%Y-%m-%d %H:%M:%S'   #errors='coerce'\n",
    "\n",
    "date_col=['Accident_Date']\n",
    "for col in date_col:\n",
    "    df[col] =  pd.to_datetime(df[col], format='%Y/%m', errors='ignore')  #'%Y-%m-%d %H:%M:%S'   #errors='coerce'\n",
    "\n",
    "date_col=['Accident_Time']\n",
    "for col in date_col:\n",
    "    df[col] =  pd.to_datetime(df[col], format='%H:%M', errors='ignore')  #'%Y-%m-%d %H:%M:%S'   #errors='coerce'\n",
    "\n",
    "df[date_cols].info(verbose= True)\n",
    "df_all = df\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(date_cols, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152685742"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351273 entries, 0 to 351272\n",
      "Columns: 699 entries, Policy_Number to 2CSD3meanIO\n",
      "dtypes: float64(557), int64(141), object(1)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "df_all = df_all.fillna(0)\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## combine Classifier result to Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Policy_Number</th>\n",
       "      <th>Next_Premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55789b8f86893761c9aa9e7bf17938e737decc68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6df13a3384528ba6339c52b4fff7c149de68011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e112d926103147bcdcb6dab201b736185a3e2520</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa346fa4b1931d1c7a55f8e1bca40b0927dd65ac</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39c4d5daaa791676ec5559c9066d7e8e8dfc51d7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Policy_Number  Next_Premium\n",
       "0  55789b8f86893761c9aa9e7bf17938e737decc68             1\n",
       "1  b6df13a3384528ba6339c52b4fff7c149de68011             1\n",
       "2  e112d926103147bcdcb6dab201b736185a3e2520             0\n",
       "3  aa346fa4b1931d1c7a55f8e1bca40b0927dd65ac             1\n",
       "4  39c4d5daaa791676ec5559c9066d7e8e8dfc51d7             1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF = pd.read_csv('../Tbrain_Insurance/Classifier.csv')     ##\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(351273, 699)\n",
      "(351273, 700)\n"
     ]
    }
   ],
   "source": [
    "# print(df_all.shape)\n",
    "# df_all = df_all.merge(DF, on = 'Policy_Number', how = 'left' ,suffixes=('','_Bi'))\n",
    "# print(df_all.shape)\n",
    "\n",
    "# print(df_all.shape)\n",
    "# df_all = df_all.drop(['Next_Premium_Bi'], axis =1) \n",
    "# print(df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(351273, 699)\n",
      "(351273, 698)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = df_all.drop(['cont_ratio'], axis =1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nature & Juridical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174910, 698)\n",
      "(120048, 698)\n",
      "294958\n",
      "294958\n",
      "(35853, 696)\n",
      "(20462, 696)\n",
      "56315\n",
      "56315\n"
     ]
    }
   ],
   "source": [
    "# Nature\n",
    "df1 = df_all[(df_all['fsex_1']==1)|(df_all['fsex_2']==1)]\n",
    "\n",
    "df_train1 = df1[df1['Next_Premium']!=100]\n",
    "print(df_train1.shape)\n",
    "df_test1 = df1[df1['Next_Premium']==100]\n",
    "print(df_test1.shape)\n",
    "print(df_train1.shape[0]+df_test1.shape[0])\n",
    "print(df1.shape[0])\n",
    "\n",
    "# Juridical\n",
    "df2 = df_all[(df_all['fsex_ ']==1)]\n",
    "df2 = df2.drop(['i_age', 'd_age'], axis=1)    ######\n",
    "\n",
    "df_train2 = df2[df2['Next_Premium']!=100]\n",
    "print(df_train2.shape)\n",
    "df_test2 = df2[df2['Next_Premium']==100]\n",
    "print(df_test2.shape)\n",
    "print(df_train2.shape[0]+df_test2.shape[0])\n",
    "print(df2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Optional \n",
    "# # downsampling\n",
    "# df_train = df_train.sample(10000).reset_index()\n",
    "# df_train = df_train[df_train.columns[1:]]\n",
    "# df_train.head()\n",
    "# # df_test = df_test.sample (1400).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     4414\n",
       "1     8236\n",
       "2     6153\n",
       "3    12135\n",
       "4     4345\n",
       "Name: Next_Premium, dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1=df_train1['Next_Premium']\n",
    "y1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34     2869\n",
       "44     1665\n",
       "47     3841\n",
       "65        0\n",
       "115    6383\n",
       "Name: Next_Premium, dtype: int64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2=df_train2['Next_Premium']\n",
    "y2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174910, 697)\n",
      "(174910,)\n",
      "(35853, 695)\n",
      "(35853,)\n"
     ]
    }
   ],
   "source": [
    "#### train_ID1 = df_train1['Policy_Number']\n",
    "df_train1 = df_train1.drop(['Policy_Number'], axis=1)\n",
    "print(df_train1.shape)\n",
    "print(train_ID1.shape)\n",
    "\n",
    "train_ID2 = df_train2['Policy_Number']\n",
    "df_train2 = df_train2.drop(['Policy_Number'], axis=1)\n",
    "print(df_train2.shape)\n",
    "print(train_ID2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120048, 697)\n",
      "(20462, 695)\n"
     ]
    }
   ],
   "source": [
    "df_test1=df_test1.drop(['Next_Premium'], axis=1)\n",
    "print(df_test1.shape)\n",
    "\n",
    "df_test2=df_test2.drop(['Next_Premium'], axis=1)\n",
    "print(df_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120048, 696)\n",
      "(120048,)\n",
      "(20462, 694)\n",
      "(20462,)\n"
     ]
    }
   ],
   "source": [
    "test_ID1=df_test1['Policy_Number']\n",
    "df_test1=df_test1.drop(['Policy_Number'], axis=1)\n",
    "print(df_test1.shape)\n",
    "print(test_ID1.shape)\n",
    "\n",
    "test_ID2=df_test2['Policy_Number']\n",
    "df_test2=df_test2.drop(['Policy_Number'], axis=1)\n",
    "print(df_test2.shape)\n",
    "print(test_ID2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1 = df_train1.drop(['Next_Premium'], axis=1)\n",
    "test1 = df_test1\n",
    "y_train1 = y1\n",
    "\n",
    "train2 = df_train2.drop(['Next_Premium'], axis=1)\n",
    "test2 = df_test2\n",
    "y_train2 = y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174910, 696)\n",
      "(120048, 696)\n",
      "(174910,)\n",
      "(35853, 694)\n",
      "(20462, 694)\n",
      "(35853,)\n"
     ]
    }
   ],
   "source": [
    "print(train1.shape)\n",
    "print(test1.shape)\n",
    "print(y1.shape)\n",
    "\n",
    "print(train2.shape)\n",
    "print(test2.shape)\n",
    "print(y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    " \n",
    "# ## standarlization\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# stdsc = StandardScaler()\n",
    "# train_std = stdsc.fit_transform(train)\n",
    "# test_std = stdsc.fit_transform(test)\n",
    " \n",
    "# X_std = train_std\n",
    "\n",
    "#     #求共變異係數矩陣\n",
    "# cov_mat = np.cov(X_std.T)\n",
    "# print(\"共變異係數矩陣.shape=\",cov_mat.shape)\n",
    "\n",
    " \n",
    "# #求共變異係數矩陣 的特徵向量及特徵值\n",
    "# eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "# print(\"特徵向量.shape=\",eigen_vecs.shape)\n",
    "\n",
    " \n",
    "# #計算解釋變異數比率 各特徵值/特徵值總和\n",
    "# tot = sum(eigen_vals)\n",
    "# var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "# cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "# # Make a list of (eigenvalue, eigenvector) tuples\n",
    "# eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n",
    "#                for i in range(len(eigen_vals))]\n",
    "\n",
    "# print(\"特徵值，特徵向量length：\",len(eigen_pairs))\n",
    "# eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n",
    "\n",
    "# #保留兩個最具影響力的特徵向量組成13x2 的投影矩陣W\n",
    "# w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "#                eigen_pairs[1][1][:, np.newaxis]))\n",
    "\n",
    "# train_pca = X_std.dot(w)\n",
    "# ###################################\n",
    "\n",
    "# X_std = test_std\n",
    "\n",
    "#     #求共變異係數矩陣\n",
    "# cov_mat = np.cov(X_std.T)\n",
    "# print(\"共變異係數矩陣.shape=\",cov_mat.shape)\n",
    "\n",
    " \n",
    "# #求共變異係數矩陣 的特徵向量及特徵值\n",
    "# eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "# print(\"特徵向量.shape=\",eigen_vecs.shape)\n",
    "\n",
    " \n",
    "# #計算解釋變異數比率 各特徵值/特徵值總和\n",
    "# tot = sum(eigen_vals)\n",
    "# var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "# cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "# # Make a list of (eigenvalue, eigenvector) tuples\n",
    "# eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n",
    "#                for i in range(len(eigen_vals))]\n",
    "\n",
    "# print(\"特徵值，特徵向量length：\",len(eigen_pairs))\n",
    "# eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n",
    "\n",
    "# #保留兩個最具影響力的特徵向量組成13x2 的投影矩陣W\n",
    "# w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "#                eigen_pairs[1][1][:, np.newaxis]))\n",
    "\n",
    "# test_pca = X_std.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = train_pca\n",
    "# test = test_pca\n",
    "# y_train = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def mae_cv1(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train1.values)\n",
    "    mae= (-cross_val_score(model, train1.values, y_train1.values, scoring=\"neg_mean_absolute_error\", cv = kf))\n",
    "    return(mae)\n",
    "\n",
    "def mae_cv2(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train2.values)\n",
    "    mae= (-cross_val_score(model, train2.values, y_train2.values, scoring=\"neg_mean_absolute_error\", cv = kf))\n",
    "    return(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # LASSO Regression :\n",
    "# # This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline :\n",
    "# lasso = make_pipeline(RobustScaler(), Lasso(alpha = 2, random_state=1))\n",
    "# print(\"lasso mae: \\n\", mae)\n",
    "# print(\"lasso score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Elastic Net Regression :\n",
    "# # again made robust to outliers:\n",
    "# #l1_list = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=2, l1_ratio=1, random_state=3))\n",
    "# mae = mae_cv(ENet)\n",
    "# print(\"ENet mae: \\n\", mae)\n",
    "# print(\"ENet score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Kernel Ridge Regression :\n",
    "# Kernel_list = ['linear','rbf','sigmoid','poly','laplacian','cosine','chi2']\n",
    "# alpha_list = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "\n",
    "# KRR = KernelRidge(alpha = 0.02, kernel = 'cosine', degree = 0) \n",
    "# # mae = mae_cv(KRR)\n",
    "# # print(\"KRR mae: \\n\", mae)\n",
    "# # print(\"KRR score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # With huber loss that makes it robust to outliers:\n",
    "# GBoost = GradientBoostingRegressor()\n",
    "# # mae = mae_cv(GBoost)\n",
    "# # print(\"GBoost mae: \\n\", mae)\n",
    "# # print(\"GBoost score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Averaged base models class\n",
    "# class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "#     def __init__(self, models):\n",
    "#         self.models = models\n",
    "        \n",
    "#     # we define clones of the original models to fit the data in\n",
    "#     def fit(self, X, y):\n",
    "#         self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "#         # Train cloned base models\n",
    "#         for model in self.models_:\n",
    "#             model.fit(X, y)\n",
    "\n",
    "#         return self\n",
    "    \n",
    "#     #Now we do the predictions for cloned models and average them\n",
    "#     def predict(self, X):\n",
    "#         predictions = np.column_stack([\n",
    "#             model.predict(X) for model in self.models_\n",
    "#         ])\n",
    "#         return np.mean(predictions, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## mae Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae = mae_cv(lasso)\n",
    "# mae_lasso = mae \n",
    "# print(\"lasso mae: \\n\", mae)\n",
    "# print(\"lasso score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lasso mae: \n",
    "#  [ 1650.25138092  2758.60603395  2507.70387202  2472.04363955  2150.09175628]\n",
    "# lasso score: 2307.7393 (381.4199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae = mae_cv(ENet)\n",
    "# mae_ENet = mae\n",
    "# print(\"ENet mae: \\n\", mae)\n",
    "# print(\"ENet score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ENet mae: \n",
    "#  [ 1647.98492773  2781.01160241  2516.92456053  2486.03839108  2164.61988768]\n",
    "# ENet score: 2319.3159 (388.4906)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae = (mae_cv(KRR))\n",
    "# mae_KRR = mae\n",
    "# print(\"KRR mae: \\n\", mae)\n",
    "# print(\"KRR score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae = mae_cv(GBoost)\n",
    "# mae_GBoost = mae\n",
    "# print(\"GBoost mae: \\n\", mae)\n",
    "# print(\"GBoost score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GBoost mae: \n",
    "#  [ 1461.50882094  2446.82567999  2218.31836897  2210.17782972  1910.665789  ]\n",
    "# GBoost score: 2049.4993 (339.7626)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Averaged base models score\n",
    "# averaged_models = AveragingModels(models = (lasso, ENet, GBoost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae = mae_cv(averaged_models)\n",
    "# mae_Avg = mae\n",
    "# print(\" Averaged base models: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Averaged base models: 2189.5514 (372.8555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Meta-Model\n",
    "# #Stacking averaged Models Class:\n",
    "# class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "#     def __init__(self, base_models, meta_model, n_folds=5):\n",
    "#         self.base_models = base_models\n",
    "#         self.meta_model = meta_model\n",
    "#         self.n_folds = n_folds\n",
    "   \n",
    "#     # We again fit the data on clones of the original models\n",
    "#     def fit(self, X, y):\n",
    "#         self.base_models_ = [list() for x in self.base_models]\n",
    "#         self.meta_model_ = clone(self.meta_model)\n",
    "#         kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "#         # Train cloned base models then create out-of-fold predictions\n",
    "#         # that are needed to train the cloned meta-model\n",
    "#         out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "#         for i, model in enumerate(self.base_models):\n",
    "#             for train_index, holdout_index in kfold.split(X, y):\n",
    "#                 instance = clone(model)\n",
    "#                 self.base_models_[i].append(instance)\n",
    "#                 instance.fit(X[train_index], y[train_index])\n",
    "#                 y_pred = instance.predict(X[holdout_index])\n",
    "#                 out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "#         # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "#         self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "#         return self\n",
    "   \n",
    "#     #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "#     #meta-features for the final prediction which is done by the meta-model\n",
    "#     def predict(self, X):\n",
    "#         meta_features = np.column_stack([\n",
    "#             np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "#             for base_models in self.base_models_ ])\n",
    "#         return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mae_1(y1, y_pred1):\n",
    "    return mean_absolute_error(y1,y_pred1)\n",
    "def mae_2(y2, y_pred2):\n",
    "    return mean_absolute_error(y2,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Stacking Averaged models Score\n",
    "\n",
    "# stacked_averaged_models = StackingAveragedModels(base_models = (ENet, lasso, GBoost),  #ENet, lasso, KRR\n",
    "#                                                  meta_model = GBoost) #GBoost\n",
    "\n",
    "# mae = mae_cv(stacked_averaged_models)\n",
    "# mae_stacked_avg = mae\n",
    "# print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stacking Averaged models score: 2042.9411 (339.1027)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# more models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# # lgb mae1: \n",
    "#  [ 1434.84210925  2260.83808605  2150.82386376  2179.23853216  1866.96877933]\n",
    "# lgb score1: 1978.5423 (302.5589)\n",
    "\n",
    "# lgb mae2: \n",
    "#  [ 3867.42438097  4014.83936575  3732.4610646   3878.76570465  3291.6854473 ]\n",
    "# lgb score1: 3757.0352 (249.2474)\n",
    "\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgb mae1: \n",
      " [ 1434.19838608  2264.13989891  2154.20454303  2189.01533884  1870.35227644]\n",
      "lgb score1: 1982.3821 (304.8487)\n",
      "\n",
      "lgb mae2: \n",
      " [ 3911.89496043  4021.96725171  3737.85396186  3880.84998805  3290.96836635]\n",
      "lgb score1: 3768.7069 (255.5017)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LGBM :\n",
    "model_lgb1 = lgb.LGBMRegressor(objective='regression')\n",
    "mae1 = mae_cv1(model_lgb1)\n",
    "mae_lgb1 = mae1\n",
    "print(\"lgb mae1: \\n\", mae1)\n",
    "print(\"lgb score1: {:.4f} ({:.4f})\\n\".format(mae1.mean(), mae1.std()))\n",
    "\n",
    "model_lgb2 = lgb.LGBMRegressor(objective='regression')\n",
    "mae2 = mae_cv2(model_lgb2)\n",
    "mae_lgb2 = mae2\n",
    "print(\"lgb mae2: \\n\", mae2)\n",
    "print(\"lgb score1: {:.4f} ({:.4f})\\n\".format(mae2.mean(), mae2.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae_lgb on train1: 1974.92916736\n",
      "Feature ranking:\n",
      "1. feature 111.: Premiumtr (66200.000000)\n",
      "2. feature 7.: pdmg_acctr (28900.000000)\n",
      "3. feature 21.: Premium_SumBtr (20900.000000)\n",
      "4. feature 189.: I_P_ratio3tr (20800.000000)\n",
      "5. feature 185.: Insured_Amount3tr (15400.000000)\n",
      "6. feature 186.: Insured_AmountOtr (10500.000000)\n",
      "7. feature 82.: Insured_Amount3_BSumtr (10000.000000)\n",
      "mae_lgb on train2: 3715.49105034\n",
      "Feature ranking:\n",
      "1. feature 109.: Premiumtr (72700.000000)\n",
      "2. feature 183.: Insured_Amount3tr (20900.000000)\n",
      "3. feature 70.: fmarriage_ tr (16000.000000)\n",
      "4. feature 189.: P_R_pctAtr (14700.000000)\n",
      "5. feature 74.: Insured_Amount1_BSumtr (12100.000000)\n",
      "6. feature 86.: Insured_Amount1_BStdtr (11500.000000)\n",
      "7. feature 80.: Insured_Amount3_BSumtr (10400.000000)\n"
     ]
    }
   ],
   "source": [
    "model = model_lgb1  #\n",
    "\n",
    "model.fit(train1, y_train1)\n",
    "train_pred_lgb1 = model.predict(train1)\n",
    "mae1 = mae_1(y_train1, train_pred_lgb1)  #\n",
    "mae_lgb_train1 = mae1                                   #\n",
    "print('mae_lgb on train1:', mae1) #\n",
    "\n",
    "y_pred_lgb1 = model.predict(test1)  #\n",
    "\n",
    "importances1 = model.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "indices1 = np.argsort(importances1)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(7):   # train.shape[1]\n",
    "    print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices1[f], train1.columns[indices1[f]], importances1[indices1[f]]*100))\n",
    "\n",
    "# # Plot the feature importances of the forest\n",
    "# plt.figure()\n",
    "# plt.title(\"Feature importances\")\n",
    "# plt.bar(range(train.shape[1]), importances[indices],\n",
    "#        color=\"r\") #, yerr=std[indices], align=\"center\")\n",
    "# plt.xticks(range(train.shape[1]), indices)\n",
    "# plt.xlim([-1, train.shape[1]])\n",
    "# plt.show()\n",
    "\n",
    "model = model_lgb2  #\n",
    "model.fit(train2, y_train2)\n",
    "train_pred_lgb2 = model.predict(train2)\n",
    "mae2 = mae_2(y_train2, train_pred_lgb2)  #\n",
    "mae_lgb_train2 = mae2                                   #\n",
    "print('mae_lgb on train2:', mae2) #\n",
    "\n",
    "y_pred_lgb2 = model.predict(test2)  #\n",
    "\n",
    "importances2 = model.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "indices2 = np.argsort(importances2)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(7):   # train.shape[1]\n",
    "    print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices2[f], train2.columns[indices2[f]], importances2[indices2[f]]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mae_lgb on train1: 1969.9914609\n",
    "Feature ranking:\n",
    "1. feature 112.: Premiumtr (64800.000000)\n",
    "2. feature 7.: pdmg_acctr (28300.000000)\n",
    "3. feature 22.: Premium_SumBtr (20200.000000)\n",
    "4. feature 190.: I_P_ratio3tr (18000.000000)\n",
    "5. feature 19.: cont_ratiotr (17900.000000)\n",
    "6. feature 186.: Insured_Amount3tr (15200.000000)\n",
    "7. feature 188.: I_P_ratio1tr (10000.000000)\n",
    "mae_lgb on train2: 3718.18953465\n",
    "Feature ranking:\n",
    "1. feature 110.: Premiumtr (73200.000000)\n",
    "2. feature 19.: cont_ratiotr (24500.000000)\n",
    "3. feature 71.: fmarriage_ tr (16400.000000)\n",
    "4. feature 184.: Insured_Amount3tr (16300.000000)\n",
    "5. feature 75.: Insured_Amount1_BSumtr (10600.000000)\n",
    "6. feature 190.: P_R_pctAtr (9600.000000)\n",
    "7. feature 81.: Insured_Amount3_BSumtr (8700.000000)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# Xgboost mae: \n",
    "#  [ 1553.09530469  2401.15759272  2177.35396934  2174.6915555   1881.35674227]\n",
    "# Xgboost score: 2037.5310 (293.1403)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-36cd903f2994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# XGBoost :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_xgb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#n_estimators=200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmae1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmae_cv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_xgb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmae_xgb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmae2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Xgboost mae1: \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-cd8fb4cb4440>\u001b[0m in \u001b[0;36mmae_cv1\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmae_cv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mkf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmae\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"neg_mean_absolute_error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model)\u001b[0m\n\u001b[1;32m    291\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                               verbose_eval=verbose, xgb_model=xgb_model)\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 898\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# XGBoost :\n",
    "model_xgb1 = xgb.XGBRegressor() #n_estimators=200\n",
    "mae1 = mae_cv1(model_xgb1)\n",
    "mae_xgb1 = mae2\n",
    "print(\"Xgboost mae1: \\n\", mae1)\n",
    "print(\"Xgboost score1: {:.4f} ({:.4f})\\n\".format(mae1.mean(), mae1.std()))\n",
    "\n",
    "model_xgb2 = xgb.XGBRegressor() #n_estimators=200\n",
    "mae2 = mae_cv2(model_xgb2)\n",
    "mae_xgb2 = mae2\n",
    "print(\"Xgboost mae2: \\n\", mae2)\n",
    "print(\"Xgboost score2: {:.4f} ({:.4f})\\n\".format(mae2.mean(), mae2.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Xgboost mae: \n",
    "#  [ 2209.30725367  2330.14260246  2082.15111059  2190.86471294  2111.35915974]\n",
    "# Xgboost score: 2184.7650 (86.8242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_xgb1  #\n",
    "\n",
    "model.fit(train1, y_train1)\n",
    "train_pred_xgb1 = model.predict(train1)  #\n",
    "mae1 = mae_1(y_train1, train_pred_xgb1)   #\n",
    "mae_xgb_train1 = mae1                                   #\n",
    "print('mae_xgb on train:', mae1) #\n",
    "\n",
    "y_pred_xgb1 = model.predict(test1)  #\n",
    "\n",
    "importances1 = model.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "indices1 = np.argsort(importances1)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(7):   # train.shape[1]\n",
    "    print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices1[f], train1.columns[indices1[f]], importances1[indices1[f]]*100))\n",
    "\n",
    "\n",
    "model = model_xgb2  #    \n",
    "model.fit(train2, y_train2)\n",
    "train_pred_xgb2 = model.predict(train2)  #\n",
    "mae2 = mae_2(y_train2, train_pred_xgb2)   #\n",
    "mae_xgb_train2 = mae2                                   #\n",
    "print('mae_xgb on train:', mae2) #\n",
    "\n",
    "y_pred_xgb2 = model.predict(test2)  #\n",
    "\n",
    "importances2 = model.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "indices2 = np.argsort(importances2)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(7):   # train.shape[1]\n",
    "    print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices2[f], train2.columns[indices2[f]], importances2[indices2[f]]*100))\n",
    "\n",
    "# # Plot the feature importances of the forest\n",
    "# plt.figure()\n",
    "# plt.title(\"Feature importances\")\n",
    "# plt.bar(range(train.shape[1]), importances[indices],\n",
    "#        color=\"r\") #, yerr=std[indices], align=\"center\")\n",
    "# plt.xticks(range(train.shape[1]), indices)\n",
    "# plt.xlim([-1, train.shape[1]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae_xgb on train: 1924.18772008\n",
    "# Feature ranking:\n",
    "# 1. feature 166.: Premiumtr (19.466248)\n",
    "# 2. feature 250.: Premium_meantr (8.163265)\n",
    "# 3. feature 258.: Premium_Pred_Addtr (5.808477)\n",
    "# 4. feature 72.: Replacement_cost_of_insured_vehicletr (4.709576)\n",
    "# 5. feature 98.: Insured_Amount1tr (4.395605)\n",
    "# 6. feature 71.: Engine_Displacement_(Cubic_Centimeter)tr (3.767661)\n",
    "# 7. feature 165.: Coverage_Deductibletr (3.453689)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# RandomForest mae: \n",
    "#  [ 1550.90146598  2462.00879027  2237.7326891   2258.35016851  1942.53006033]\n",
    "\n",
    "# RandomForest score: 2090.3046 (316.5222)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # RF :\n",
    "# model_rf = RandomForestRegressor(n_estimators=100)  # default n_estimators = 10\n",
    "# mae = mae_cv(model_rf)\n",
    "# mae_rf = mae\n",
    "# print(\"RandomForest mae: \\n\", mae)\n",
    "# print(\"\\nRandomForest score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RandomForest mae: \n",
    "#  [ 1487.6104638   2423.44725241  2203.43303404  2212.03221765  1904.53282157]\n",
    "\n",
    "# RandomForest score: 2046.2112 (324.5396)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = model_rf  #\n",
    "\n",
    "# model.fit(train, y_train)\n",
    "# train_pred_rf = model.predict(train)  #\n",
    "# mae = mae_(y_train, train_pred_rf)   #\n",
    "# mae_rf_train = mae                                   #\n",
    "# print('mae_rf on train:', mae) #\n",
    "\n",
    "# y_pred_rf = model.predict(test)  #\n",
    "\n",
    "# importances = model.feature_importances_\n",
    "# #std = np.std([tree.feature_importances_ for tree in model_rf.estimators_], axis=0)\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# # Print the feature ranking\n",
    "# print(\"Feature ranking:\")\n",
    "\n",
    "# for f in range(7):\n",
    "#     print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices[f], train.columns[indices[f]], importances[indices[f]]*100))\n",
    "\n",
    "# # # Plot the feature importances of the forest\n",
    "# # plt.figure()\n",
    "# # plt.title(\"Feature importances\")\n",
    "# # plt.bar(range(train.shape[1]), importances[indices],\n",
    "# #        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "# # plt.xticks(range(train.shape[1]), indices)\n",
    "# # plt.xlim([-1, train.shape[1]])\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae_rf on train: 801.865598\n",
    "# Feature ranking:\n",
    "# 1. feature 166.: Premiumtr (29.192158)\n",
    "# 2. feature 167.: Premium_Predtr (26.174226)\n",
    "# 3. feature 258.: Premium_Pred_Addtr (8.405166)\n",
    "# 4. feature 72.: Replacement_cost_of_insured_vehicletr (5.817675)\n",
    "# 5. feature 71.: Engine_Displacement_(Cubic_Centimeter)tr (1.952259)\n",
    "# 6. feature 242.: Insured_Amount1_meantr (1.222075)\n",
    "# 7. feature 78.: aassured_ziptr (1.193084)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Final Training and Prediction: \n",
    "# stacked_averaged_models.fit(train.values, y_train.values)\n",
    "# stacked_train_pred = stacked_averaged_models.predict(train.values)\n",
    "# stacked_pred = stacked_averaged_models.predict(test.values)\n",
    "\n",
    "# print(mae_(y_train, stacked_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = 0.3\n",
    "x = 0.7\n",
    "# r = 0.11\n",
    "# s = 1- l - x - r\n",
    "# ensemble_train = stacked_train_pred * s + train_pred_lgb *l + train_pred_xgb* x + train_pred_rf * r\n",
    "# ensemble_test = stacked_pred * s + y_pred_lgb * l + y_pred_xgb * x + y_pred_rf *r\n",
    "\n",
    "ensemble_train1 =  train_pred_lgb1 *l + train_pred_xgb1* x \n",
    "ensemble_test1 =  y_pred_lgb1 * l + y_pred_xgb1 * x \n",
    "\n",
    "ensemble_train2 =  train_pred_lgb2 *l + train_pred_xgb2* x \n",
    "ensemble_test2 =  y_pred_lgb2 * l + y_pred_xgb2 * x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE score on train data1:')  # 1844.5\n",
    "print(mae_1(y_train1, ensemble_train1))\n",
    "\n",
    "print('MAE score on train data2:')  # 1844.5\n",
    "print(mae_2(y_train2, ensemble_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## prep submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_stacked = pd.DataFrame(stacked_pred)\n",
    "y_pred_lgb1 = pd.DataFrame(y_pred_lgb1)\n",
    "y_pred_xgb1 = pd.DataFrame(y_pred_xgb1)\n",
    "# y_pred_rf = pd.DataFrame(y_pred_rf)\n",
    "\n",
    "y_pred1 = pd.DataFrame(ensemble_test1)\n",
    "\n",
    "all_y1 = pd.concat([y_pred1,  y_pred_lgb1, y_pred_xgb1], axis=1)\n",
    "all_y1.columns=['Final', 'lgb','xgb']\n",
    "all_y1.to_csv('../Tbrain_Insurance/Reg_y1.csv')\n",
    "all_y1.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_stacked = pd.DataFrame(stacked_pred)\n",
    "y_pred_lgb2 = pd.DataFrame(y_pred_lgb2)\n",
    "y_pred_xgb2 = pd.DataFrame(y_pred_xgb2)\n",
    "# y_pred_rf = pd.DataFrame(y_pred_rf)\n",
    "\n",
    "y_pred2 = pd.DataFrame(ensemble_test2)\n",
    "\n",
    "all_y2 = pd.concat([y_pred2,  y_pred_lgb2, y_pred_xgb2], axis=1)\n",
    "all_y2.columns=['Final', 'lgb','xgb']\n",
    "all_y2.to_csv('../Tbrain_Insurance/Reg_y2.csv')\n",
    "all_y2.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_y1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_y2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative -> 0 \n",
    "b = all_y1[all_y1['Final']<0].shape[0]\n",
    "a = b/all_y1.shape[0]*100\n",
    "print((\"negative prediction1 (#): %2d\"%b))\n",
    "print(\"negative prediction1 (Percent): %2f\"%a)\n",
    "\n",
    "b = all_y2[all_y2['Final']<0].shape[0]\n",
    "a = b/all_y2.shape[0]*100\n",
    "print((\"negative prediction2 (#): %2d\"%b))\n",
    "print(\"negative prediction2 (Percent): %2f\"%a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# condition1 = all_y['Final']<0\n",
    "# all_y['Final'].loc[condition1] = 0\n",
    "# all_y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_testing.shape)\n",
    "print(test_ID1.shape)\n",
    "print(y_pred1.shape)\n",
    "print(test_ID2.shape)\n",
    "print(y_pred2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(test_ID1).reset_index()\n",
    "# df1['Next_Premium'] = y_pred_lgb1\n",
    "df1['Next_Premium'] = all_y1['Final']  ##\n",
    "\n",
    "df2 = pd.DataFrame(test_ID2).reset_index()\n",
    "# df2['Next_Premium'] = y_pred_lgb2\n",
    "df2['Next_Premium'] = all_y2['Final'] ##\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df = df_testing.merge(df, how = 'left', on = 'Policy_Number', suffixes=('_x','_y'))\n",
    "DF = df.drop(['index','Next_Premium_x'], axis = 1)\n",
    "DF.columns = df_testing.columns\n",
    "DF.to_csv('../Tbrain_Insurance/testing-set_en.csv',index = False)     ##\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(test_ID1).reset_index()\n",
    "# df1['Next_Premium'] = y_pred_lgb1\n",
    "df1['Next_Premium'] = all_y1['lgb']  ##\n",
    "\n",
    "df2 = pd.DataFrame(test_ID2).reset_index()\n",
    "# df2['Next_Premium'] = y_pred_lgb2\n",
    "df2['Next_Premium'] = all_y2['lgb'] ##\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df.head()\n",
    "\n",
    "df = df_testing.merge(df, how = 'left', on = 'Policy_Number', suffixes=('_x','_y'))\n",
    "DF = df.drop(['index','Next_Premium_x'], axis = 1)\n",
    "DF.columns = df_testing.columns\n",
    "DF.to_csv('../Tbrain_Insurance/testing-set_lgb.csv',index = False)     ##\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(test_ID1).reset_index()\n",
    "# df1['Next_Premium'] = y_pred_lgb1\n",
    "df1['Next_Premium'] = all_y1['xgb']  ##\n",
    "\n",
    "df2 = pd.DataFrame(test_ID2).reset_index()\n",
    "# df2['Next_Premium'] = y_pred_lgb2\n",
    "df2['Next_Premium'] = all_y2['xgb'] ##\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df = df_testing.merge(df, how = 'left', on = 'Policy_Number', suffixes=('_x','_y'))\n",
    "DF = df.drop(['index','Next_Premium_x'], axis = 1)\n",
    "DF.columns = df_testing.columns\n",
    "DF.to_csv('../Tbrain_Insurance/testing-set_xgb.csv',index = False)     ##\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
