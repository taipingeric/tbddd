{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_testing = pd.read_csv('../Tbrain_Insurance/testing-set.csv')\n",
    "#df_all = pd.read_csv('../Tbrain_Insurance/df_all.csv')\n",
    "df_all = pd.read_csv('../Tbrain_Insurance/df_policy_claim_unuique.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351273, 704)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "19\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351273 entries, 0 to 351272\n",
      "Columns: 704 entries, Policy_Number to 2CSD3meanIO\n",
      "dtypes: float64(557), int64(141), object(6)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "# replace target: \n",
    "replace_date = {'09/2658','10/2622','02/2611','11/2602', '10/2582', '10/2581', '04/2579', '08/2569', '10/2521',\n",
    " '11/2512', '01/2512', '10/2472','08/2471', '11/2464', '03/2464', '12/2453', '01/2442', '07/2280', '02/2152'}\n",
    "print(len(replace_date))\n",
    "replace_date_set = set(replace_date)\n",
    "print(len(replace_date_set))\n",
    "\n",
    "## convert to datetime formate\n",
    "date_cols = ['DOB_of_Driver','Accident_Date','Accident_Time','ibirth', 'dbirth_new']\n",
    "\n",
    "\n",
    "for input1 in replace_date_set:\n",
    "    df_all[date_cols] = df_all[date_cols].replace(input1, pd.to_datetime('01/2017', format ='%m/%Y'))\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOB_of_Driver</th>\n",
       "      <th>Accident_Date</th>\n",
       "      <th>Accident_Time</th>\n",
       "      <th>ibirth</th>\n",
       "      <th>dbirth_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1980-11-01</td>\n",
       "      <td>1980-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1956-11-01</td>\n",
       "      <td>1982-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1954-03-01</td>\n",
       "      <td>1954-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1978-09-01</td>\n",
       "      <td>1978-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>1960-10-01</td>\n",
       "      <td>1960-10-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DOB_of_Driver        Accident_Date        Accident_Time      ibirth  \\\n",
       "0  2017-01-01 00:00:00  2017-01-01 00:00:00  2017-01-01 00:00:00  1980-11-01   \n",
       "1  2017-01-01 00:00:00  2017-01-01 00:00:00  2017-01-01 00:00:00  1956-11-01   \n",
       "2  2017-01-01 00:00:00  2017-01-01 00:00:00  2017-01-01 00:00:00  1954-03-01   \n",
       "3  2017-01-01 00:00:00  2017-01-01 00:00:00  2017-01-01 00:00:00  1978-09-01   \n",
       "4  2017-01-01 00:00:00  2017-01-01 00:00:00  2017-01-01 00:00:00  1960-10-01   \n",
       "\n",
       "   dbirth_new  \n",
       "0  1980-11-01  \n",
       "1  1982-03-01  \n",
       "2  1954-03-01  \n",
       "3  1978-09-01  \n",
       "4  1960-10-01  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fill datetime value for date_cols \n",
    "df_all[date_cols] = df_all[date_cols].fillna(pd.to_datetime('01/2017', format ='%m/%Y'))\n",
    "df_all[date_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351273 entries, 0 to 351272\n",
      "Data columns (total 5 columns):\n",
      "DOB_of_Driver    351273 non-null datetime64[ns]\n",
      "Accident_Date    351273 non-null datetime64[ns]\n",
      "Accident_Time    351273 non-null datetime64[ns]\n",
      "ibirth           351273 non-null datetime64[ns]\n",
      "dbirth_new       351273 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](5)\n",
      "memory usage: 13.4 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351273 entries, 0 to 351272\n",
      "Columns: 704 entries, Policy_Number to 2CSD3meanIO\n",
      "dtypes: datetime64[ns](5), float64(557), int64(141), object(1)\n",
      "memory usage: 1.8+ GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "152685742"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert to datetime formate\n",
    "date_cols = ['DOB_of_Driver','Accident_Date','Accident_Time','ibirth', 'dbirth_new']\n",
    "\n",
    "df = df_all\n",
    "\n",
    "date_col=['ibirth', 'dbirth_new']\n",
    "for col in date_col:\n",
    "    df[col] =  pd.to_datetime(df[col], format='%Y-%m-%d', errors='ignore')  #'%Y-%m-%d %H:%M:%S'   #errors='coerce'\n",
    "\n",
    "\n",
    "    \n",
    "date_col=['DOB_of_Driver', 'ibirth', 'dbirth_new']\n",
    "for col in date_col:\n",
    "    df[col] =  pd.to_datetime(df[col], format='%m/%Y', errors='ignore')  #'%Y-%m-%d %H:%M:%S'   #errors='coerce'\n",
    "\n",
    "date_col=['Accident_Date']\n",
    "for col in date_col:\n",
    "    df[col] =  pd.to_datetime(df[col], format='%Y/%m', errors='ignore')  #'%Y-%m-%d %H:%M:%S'   #errors='coerce'\n",
    "\n",
    "date_col=['Accident_Time']\n",
    "for col in date_col:\n",
    "    df[col] =  pd.to_datetime(df[col], format='%H:%M', errors='ignore')  #'%Y-%m-%d %H:%M:%S'   #errors='coerce'\n",
    "\n",
    "df[date_cols].info(verbose= True)\n",
    "df_all = df\n",
    "\n",
    "df.info()\n",
    "df = df.drop(date_cols, axis =1)\n",
    "df_all = df\n",
    "df_all.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351273 entries, 0 to 351272\n",
      "Columns: 699 entries, Policy_Number to 2CSD3meanIO\n",
      "dtypes: float64(557), int64(141), object(1)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "df_all = df_all.fillna(0)\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.drop(['cont_ratio'], axis =1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174910, 698)\n",
      "(120048, 698)\n",
      "294958\n",
      "294958\n",
      "(35853, 696)\n",
      "(20462, 696)\n",
      "56315\n",
      "56315\n"
     ]
    }
   ],
   "source": [
    "# Nature\n",
    "df1 = df_all[(df_all['fsex_1']==1)|(df_all['fsex_2']==1)]\n",
    "\n",
    "df_train1 = df1[df1['Next_Premium']!=100]\n",
    "print(df_train1.shape)\n",
    "df_test1 = df1[df1['Next_Premium']==100]\n",
    "print(df_test1.shape)\n",
    "print(df_train1.shape[0]+df_test1.shape[0])\n",
    "print(df1.shape[0])\n",
    "\n",
    "# Juridical\n",
    "df2 = df_all[(df_all['fsex_ ']==1)]\n",
    "df2 = df2.drop(['i_age', 'd_age'], axis=1)    ######\n",
    "\n",
    "df_train2 = df2[df2['Next_Premium']!=100]\n",
    "print(df_train2.shape)\n",
    "df_test2 = df2[df2['Next_Premium']==100]\n",
    "print(df_test2.shape)\n",
    "print(df_train2.shape[0]+df_test2.shape[0])\n",
    "print(df2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Optional \n",
    "# # downsampling\n",
    "# df_train = df_train.sample(10000).reset_index()\n",
    "# df_train = df_train[df_train.columns[1:]]\n",
    "# df_train.head()\n",
    "# # df_test = df_test.sample (1400).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ching/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/ching/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_train1['Next_Premium'] = df_train1.apply(lambda row: 0 if row['Next_Premium']==0 else 1, axis =1)\n",
    "df_train2['Next_Premium'] = df_train2.apply(lambda row: 0 if row['Next_Premium']==0 else 1, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue 78.04070664913384\n",
      "cancelled 21.95929335086616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: Next_Premium, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1=df_train1['Next_Premium']\n",
    "print('continue',df_train1[df_train1['Next_Premium']==1].shape[0]/df_train1.shape[0]*100)\n",
    "print('cancelled',df_train1[df_train1['Next_Premium']==0].shape[0]/df_train1.shape[0]*100)\n",
    "y1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue 78.24170920146153\n",
      "cancelled 21.758290798538475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34     1\n",
       "44     1\n",
       "47     1\n",
       "65     0\n",
       "115    1\n",
       "Name: Next_Premium, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = df_train2['Next_Premium']\n",
    "print('continue',df_train2[df_train2['Next_Premium']==1].shape[0]/df_train2.shape[0]*100)\n",
    "print('cancelled',df_train2[df_train2['Next_Premium']==0].shape[0]/df_train2.shape[0]*100)\n",
    "y2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174910, 697)\n",
      "(174910,)\n",
      "(35853, 695)\n",
      "(35853,)\n"
     ]
    }
   ],
   "source": [
    "train_ID1 = df_train1['Policy_Number']\n",
    "df_train1 = df_train1.drop(['Policy_Number'], axis=1)\n",
    "print(df_train1.shape)\n",
    "print(train_ID1.shape)\n",
    "\n",
    "train_ID2 = df_train2['Policy_Number']\n",
    "df_train2 = df_train2.drop(['Policy_Number'], axis=1)\n",
    "print(df_train2.shape)\n",
    "print(train_ID2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120048, 697)\n",
      "(20462, 695)\n"
     ]
    }
   ],
   "source": [
    "df_test1=df_test1.drop(['Next_Premium'], axis=1)\n",
    "print(df_test1.shape)\n",
    "df_test2=df_test2.drop(['Next_Premium'], axis=1)\n",
    "print(df_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120048, 696)\n",
      "(120048,)\n",
      "(20462, 694)\n",
      "(20462,)\n"
     ]
    }
   ],
   "source": [
    "test_ID1=df_test1['Policy_Number']\n",
    "df_test1=df_test1.drop(['Policy_Number'], axis=1)\n",
    "print(df_test1.shape)\n",
    "print(test_ID1.shape)\n",
    "\n",
    "test_ID2=df_test2['Policy_Number']\n",
    "df_test2=df_test2.drop(['Policy_Number'], axis=1)\n",
    "print(df_test2.shape)\n",
    "print(test_ID2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1 = df_train1.drop(['Next_Premium'], axis=1)\n",
    "test1 = df_test1\n",
    "y_train1 = y1\n",
    "\n",
    "train2 = df_train2.drop(['Next_Premium'], axis=1)\n",
    "test2 = df_test2\n",
    "y_train2 = y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174910, 696)\n",
      "(120048, 696)\n",
      "(174910,)\n",
      "(35853, 694)\n",
      "(20462, 694)\n",
      "(35853,)\n"
     ]
    }
   ],
   "source": [
    "print(train1.shape)\n",
    "print(test1.shape)\n",
    "print(y1.shape)\n",
    "\n",
    "print(train2.shape)\n",
    "print(test2.shape)\n",
    "print(y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    " \n",
    "# ## standarlization\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# stdsc = StandardScaler()\n",
    "# train_std = stdsc.fit_transform(train)\n",
    "# test_std = stdsc.fit_transform(test)\n",
    " \n",
    "# X_std = train_std\n",
    "\n",
    "#     #求共變異係數矩陣\n",
    "# cov_mat = np.cov(X_std.T)\n",
    "# print(\"共變異係數矩陣.shape=\",cov_mat.shape)\n",
    "\n",
    " \n",
    "# #求共變異係數矩陣 的特徵向量及特徵值\n",
    "# eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "# print(\"特徵向量.shape=\",eigen_vecs.shape)\n",
    "\n",
    " \n",
    "# #計算解釋變異數比率 各特徵值/特徵值總和\n",
    "# tot = sum(eigen_vals)\n",
    "# var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "# cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "# # Make a list of (eigenvalue, eigenvector) tuples\n",
    "# eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n",
    "#                for i in range(len(eigen_vals))]\n",
    "\n",
    "# print(\"特徵值，特徵向量length：\",len(eigen_pairs))\n",
    "# eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n",
    "\n",
    "# #保留兩個最具影響力的特徵向量組成13x2 的投影矩陣W\n",
    "# w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "#                eigen_pairs[1][1][:, np.newaxis]))\n",
    "\n",
    "# train_pca = X_std.dot(w)\n",
    "# ###################################\n",
    "\n",
    "# X_std = test_std\n",
    "\n",
    "#     #求共變異係數矩陣\n",
    "# cov_mat = np.cov(X_std.T)\n",
    "# print(\"共變異係數矩陣.shape=\",cov_mat.shape)\n",
    "\n",
    " \n",
    "# #求共變異係數矩陣 的特徵向量及特徵值\n",
    "# eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "# print(\"特徵向量.shape=\",eigen_vecs.shape)\n",
    "\n",
    " \n",
    "# #計算解釋變異數比率 各特徵值/特徵值總和\n",
    "# tot = sum(eigen_vals)\n",
    "# var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "# cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "# # Make a list of (eigenvalue, eigenvector) tuples\n",
    "# eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n",
    "#                for i in range(len(eigen_vals))]\n",
    "\n",
    "# print(\"特徵值，特徵向量length：\",len(eigen_pairs))\n",
    "# eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n",
    "\n",
    "# #保留兩個最具影響力的特徵向量組成13x2 的投影矩陣W\n",
    "# w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "#                eigen_pairs[1][1][:, np.newaxis]))\n",
    "\n",
    "# test_pca = X_std.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = train_pca\n",
    "# test = test_pca\n",
    "# y_train = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def mae_cv1(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train1.values)\n",
    "    mae= (cross_val_score(model, train1.values, y_train1.values, scoring=\"accuracy\", cv = kf))\n",
    "    return(mae)\n",
    "\n",
    "def mae_cv2(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train2.values)\n",
    "    mae= (cross_val_score(model, train2.values, y_train2.values, scoring=\"accuracy\", cv = kf))\n",
    "    return(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # LASSO Regression :\n",
    "# # This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline :\n",
    "# lasso = make_pipeline(RobustScaler(), Lasso(alpha = 2, random_state=1))\n",
    "# print(\"lasso mae: \\n\", mae)\n",
    "# print(\"lasso score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Elastic Net Regression :\n",
    "# # again made robust to outliers:\n",
    "# #l1_list = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=2, l1_ratio=1, random_state=3))\n",
    "# mae = mae_cv(ENet)\n",
    "# print(\"ENet mae: \\n\", mae)\n",
    "# print(\"ENet score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Kernel Ridge Regression :\n",
    "# Kernel_list = ['linear','rbf','sigmoid','poly','laplacian','cosine','chi2']\n",
    "# alpha_list = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "\n",
    "# KRR = KernelRidge(alpha = 0.02, kernel = 'cosine', degree = 0) \n",
    "# # mae = mae_cv(KRR)\n",
    "# # print(\"KRR mae: \\n\", mae)\n",
    "# # print(\"KRR score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # With huber loss that makes it robust to outliers:\n",
    "# GBoost = GradientBoostingRegressor()\n",
    "# # mae = mae_cv(GBoost)\n",
    "# # print(\"GBoost mae: \\n\", mae)\n",
    "# # print(\"GBoost score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Averaged base models class\n",
    "# class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "#     def __init__(self, models):\n",
    "#         self.models = models\n",
    "        \n",
    "#     # we define clones of the original models to fit the data in\n",
    "#     def fit(self, X, y):\n",
    "#         self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "#         # Train cloned base models\n",
    "#         for model in self.models_:\n",
    "#             model.fit(X, y)\n",
    "\n",
    "#         return self\n",
    "    \n",
    "#     #Now we do the predictions for cloned models and average them\n",
    "#     def predict(self, X):\n",
    "#         predictions = np.column_stack([\n",
    "#             model.predict(X) for model in self.models_\n",
    "#         ])\n",
    "#         return np.mean(predictions, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## mae Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae = mae_cv(lasso)\n",
    "# mae_lasso = mae \n",
    "# print(\"lasso mae: \\n\", mae)\n",
    "# print(\"lasso score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae = mae_cv(ENet)\n",
    "# mae_ENet = mae\n",
    "# print(\"ENet mae: \\n\", mae)\n",
    "# print(\"ENet score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae = (mae_cv(KRR))\n",
    "# mae_KRR = mae\n",
    "# print(\"KRR mae: \\n\", mae)\n",
    "# print(\"KRR score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae = mae_cv(GBoost)\n",
    "# mae_GBoost = mae\n",
    "# print(\"GBoost mae: \\n\", mae)\n",
    "# print(\"GBoost score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Averaged base models score\n",
    "# averaged_models = AveragingModels(models = (lasso, ENet, GBoost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae = mae_cv(averaged_models)\n",
    "# mae_Avg = mae\n",
    "# print(\" Averaged base models: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Meta-Model\n",
    "# #Stacking averaged Models Class:\n",
    "# class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "#     def __init__(self, base_models, meta_model, n_folds=5):\n",
    "#         self.base_models = base_models\n",
    "#         self.meta_model = meta_model\n",
    "#         self.n_folds = n_folds\n",
    "   \n",
    "#     # We again fit the data on clones of the original models\n",
    "#     def fit(self, X, y):\n",
    "#         self.base_models_ = [list() for x in self.base_models]\n",
    "#         self.meta_model_ = clone(self.meta_model)\n",
    "#         kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "#         # Train cloned base models then create out-of-fold predictions\n",
    "#         # that are needed to train the cloned meta-model\n",
    "#         out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "#         for i, model in enumerate(self.base_models):\n",
    "#             for train_index, holdout_index in kfold.split(X, y):\n",
    "#                 instance = clone(model)\n",
    "#                 self.base_models_[i].append(instance)\n",
    "#                 instance.fit(X[train_index], y[train_index])\n",
    "#                 y_pred = instance.predict(X[holdout_index])\n",
    "#                 out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "#         # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "#         self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "#         return self\n",
    "   \n",
    "#     #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "#     #meta-features for the final prediction which is done by the meta-model\n",
    "#     def predict(self, X):\n",
    "#         meta_features = np.column_stack([\n",
    "#             np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "#             for base_models in self.base_models_ ])\n",
    "#         return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mae_1(y1, y_pred1):\n",
    "    return mean_absolute_error(y1,y_pred1)\n",
    "def mae_2(y2, y_pred2):\n",
    "    return mean_absolute_error(y2,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Stacking Averaged models Score\n",
    "\n",
    "# stacked_averaged_models = StackingAveragedModels(base_models = (ENet, lasso, GBoost),  #ENet, lasso, KRR\n",
    "#                                                  meta_model = GBoost) #GBoost\n",
    "\n",
    "# mae = mae_cv(stacked_averaged_models)\n",
    "# mae_stacked_avg = mae\n",
    "# print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# more models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlgb mae1: \\n [ 0.8240574   0.81833514  0.80475673  0.7899777   0.82836397]\\nlgb score1: 0.8131 (0.0140)\\n\\nlgb mae2: \\n [ 0.84523146  0.83907405  0.83598326  0.8251046   0.83417015]\\nlgb score2: 0.8359 (0.0066)\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lgb.LGBMClassifier?\n",
    "'''\n",
    "lgb mae1: \n",
    " [ 0.8240574   0.81833514  0.80475673  0.7899777   0.82836397]\n",
    "lgb score1: 0.8131 (0.0140)\n",
    "\n",
    "lgb mae2: \n",
    " [ 0.84523146  0.83907405  0.83598326  0.8251046   0.83417015]\n",
    "lgb score2: 0.8359 (0.0066)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgb mae1: \n",
      " [ 0.82325701  0.82082214  0.81256075  0.81278943  0.81744376]\n",
      "lgb score1: 0.8174 (0.0043)\n",
      "\n",
      "lgb mae2: \n",
      " [ 0.84481316  0.83642449  0.82733612  0.82426778  0.8320781 ]\n",
      "lgb score2: 0.8330 (0.0072)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LGBM :\n",
    "model_lgb1 = lgb.LGBMClassifier()\n",
    "mae1 = mae_cv1(model_lgb1)\n",
    "mae_lgb1 = mae1\n",
    "print(\"lgb mae1: \\n\", mae1)\n",
    "print(\"lgb score1: {:.4f} ({:.4f})\\n\".format(mae1.mean(), mae1.std()))\n",
    "\n",
    "model_lgb2 = lgb.LGBMClassifier()\n",
    "mae2 = mae_cv2(model_lgb2)\n",
    "mae_lgb2 = mae2\n",
    "print(\"lgb mae2: \\n\", mae2)\n",
    "print(\"lgb score2: {:.4f} ({:.4f})\\n\".format(mae2.mean(), mae2.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking1:\n",
      "1. feature 18.: V_agetr (14100.000000)\n",
      "2. feature 73.: i_agetr (12700.000000)\n",
      "3. feature 182.: Premium_meantr (11400.000000)\n",
      "4. feature 74.: d_agetr (11200.000000)\n",
      "5. feature 1.: Engine_Displacement_(Cubic_Centimeter)tr (10200.000000)\n",
      "6. feature 185.: Insured_Amount3tr (10200.000000)\n",
      "7. feature 187.: I_P_ratio1tr (9800.000000)\n",
      "8. feature 3.: Replacement_cost_of_insured_vehicletr (9000.000000)\n",
      "9. feature 191.: P_R_pctAtr (6900.000000)\n",
      "10. feature 194.: P_R_pctOtr (6600.000000)\n",
      "11. feature 190.: I_P_ratioOtr (5900.000000)\n",
      "12. feature 189.: I_P_ratio3tr (5900.000000)\n",
      "13. feature 183.: Insured_Amount1tr (5700.000000)\n",
      "14. feature 111.: Premiumtr (5400.000000)\n",
      "15. feature 211.: Premium_stdtr (5300.000000)\n",
      "Feature ranking2:\n",
      "1. feature 1.: Engine_Displacement_(Cubic_Centimeter)tr (11400.000000)\n",
      "2. feature 183.: Insured_Amount3tr (1200.000000)\n",
      "3. feature 18.: V_agetr (4200.000000)\n",
      "4. feature 30.: Num_GroupA_onIIDtr (1400.000000)\n",
      "5. feature 3.: Replacement_cost_of_insured_vehicletr (11900.000000)\n",
      "6. feature 185.: I_P_ratio1tr (8100.000000)\n",
      "7. feature 180.: Premium_meantr (7500.000000)\n",
      "8. feature 189.: P_R_pctAtr (8700.000000)\n",
      "9. feature 187.: I_P_ratio3tr (6300.000000)\n",
      "10. feature 29.: Num_GroupB_onIIDtr (1800.000000)\n",
      "11. feature 4.: Multiple_Products_with_TmNewa_(Yes_or_No?)tr (3500.000000)\n",
      "12. feature 188.: I_P_ratioOtr (7600.000000)\n",
      "13. feature 192.: P_R_pctOtr (11600.000000)\n",
      "14. feature 191.: P_R_pctCtr (0.000000)\n",
      "15. feature 184.: Insured_AmountOtr (1000.000000)\n"
     ]
    }
   ],
   "source": [
    "model = model_lgb1  #\n",
    "\n",
    "# Nature\n",
    "model.fit(train1, y_train1)\n",
    "train_pred_lgb1 = model.predict(train1)\n",
    "\n",
    "y_pred_lgb1 = model.predict(test1)  #\n",
    "\n",
    "importances1 = model.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "indices1 = np.argsort(importances1)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking1:\")\n",
    "\n",
    "for f in range(15):   # train.shape[1]\n",
    "    print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices1[f], train1.columns[indices1[f]], importances1[indices1[f]]*100))\n",
    "\n",
    "# Juridical  \n",
    "model = model_lgb2 #\n",
    "\n",
    "model.fit(train2, y_train2)\n",
    "train_pred_lgb2 = model.predict(train2)\n",
    "\n",
    "y_pred_lgb2 = model.predict(test2)  #\n",
    "\n",
    "importances2 = model.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "indices2 = np.argsort(importances2)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking2:\")\n",
    "\n",
    "for f in range(15):   # train.shape[1]\n",
    "    print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices2[f], train2.columns[indices2[f]], importances2[indices1[f]]*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost mae1: \n",
      " [ 0.82345711  0.82342347  0.81524784  0.81581956  0.81761528]\n",
      "Xgboost score1: 0.8191 (0.0036)\n",
      "\n",
      "Xgboost mae2: \n",
      " [ 0.84439487  0.83740064  0.83026499  0.82357043  0.8320781 ]\n",
      "Xgboost score2: 0.8335 (0.0070)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost :\n",
    "model_xgb1 = xgb.XGBClassifier() #n_estimators=200\n",
    "mae1 = mae_cv1(model_xgb1)\n",
    "mae_xgb1 = mae1 \n",
    "print(\"Xgboost mae1: \\n\", mae1)\n",
    "print(\"Xgboost score1: {:.4f} ({:.4f})\\n\".format(mae1.mean(), mae1.std()))\n",
    "\n",
    "model_xgb2 = xgb.XGBClassifier() #n_estimators=200\n",
    "mae2 = mae_cv2(model_xgb2)\n",
    "mae_xgb2 = mae2\n",
    "print(\"Xgboost mae2: \\n\", mae2)\n",
    "print(\"Xgboost score2: {:.4f} ({:.4f})\\n\".format(mae2.mean(), mae2.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking1:\n",
      "1. feature 18.: V_agetr (6.821480)\n",
      "2. feature 185.: Insured_Amount3tr (5.950653)\n",
      "3. feature 182.: Premium_meantr (5.805515)\n",
      "4. feature 187.: I_P_ratio1tr (3.918723)\n",
      "5. feature 1.: Engine_Displacement_(Cubic_Centimeter)tr (3.628447)\n",
      "6. feature 73.: i_agetr (3.628447)\n",
      "7. feature 74.: d_agetr (3.338171)\n",
      "Feature ranking2:\n",
      "1. feature 18.: V_agetr (5.819296)\n",
      "2. feature 187.: I_P_ratio3tr (5.666156)\n",
      "3. feature 30.: Num_GroupA_onIIDtr (5.666156)\n",
      "4. feature 28.: Num_GroupC_onIIDtr (4.441041)\n",
      "5. feature 191.: P_R_pctCtr (3.981623)\n",
      "6. feature 1.: Engine_Displacement_(Cubic_Centimeter)tr (3.828484)\n",
      "7. feature 4.: Multiple_Products_with_TmNewa_(Yes_or_No?)tr (3.522205)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "# Nature\n",
    "\n",
    "model = model_xgb1 #\n",
    "\n",
    "model.fit(train1, y_train1)\n",
    "train_pred_xgb1 = model.predict(train1)  #\n",
    "\n",
    "y_pred_xgb1 = model.predict(test1)  #\n",
    "\n",
    "importances1 = model.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "indices1 = np.argsort(importances1)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking1:\")\n",
    "\n",
    "for f in range(7):   # train.shape[1]\n",
    "    print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices1[f], train1.columns[indices1[f]], importances1[indices1[f]]*100))\n",
    "\n",
    "# Juridical\n",
    "\n",
    "model = model_xgb2  #\n",
    "\n",
    "model.fit(train2, y_train2)\n",
    "train_pred_xgb2 = model.predict(train2)  #\n",
    "\n",
    "y_pred_xgb2 = model.predict(test2)  #\n",
    "\n",
    "importances2 = model.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "indices2 = np.argsort(importances2)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking2:\")\n",
    "\n",
    "for f in range(7):   # train.shape[1]\n",
    "    print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices2[f], train2.columns[indices2[f]], importances2[indices2[f]]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # RF :\n",
    "# model_rf = RandomForestRegressor(n_estimators=100)  # default n_estimators = 10\n",
    "# mae1 = mae_cv1(model_rf)\n",
    "# mae_rf1 = mae1\n",
    "# print(\"RandomForest mae1: \\n\", mae1)\n",
    "# print(\"\\nRandomForest score1: {:.4f} ({:.4f})\\n\".format(mae1.mean(), mae1.std()))\n",
    "\n",
    "# mae2 = mae_cv2(model_rf)\n",
    "# mae_rf2 = mae2\n",
    "# print(\"RandomForest mae2: \\n\", mae2)\n",
    "# print(\"\\nRandomForest score2: {:.4f} ({:.4f})\\n\".format(mae2.mean(), mae2.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = model_rf  #\n",
    "\n",
    "# model.fit(train1, y_train1)\n",
    "# train_pred_rf1 = model.predict(train1)  #\n",
    "\n",
    "# y_pred_rf1 = model.predict(test1)  #\n",
    "\n",
    "# importances1 = model.feature_importances_\n",
    "# #std = np.std([tree.feature_importances_ for tree in model_rf.estimators_], axis=0)\n",
    "# indices1 = np.argsort(importances1)[::-1]\n",
    "\n",
    "# # Print the feature ranking\n",
    "# print(\"Feature ranking1:\")\n",
    "\n",
    "# for f in range(7):\n",
    "#     print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices1[f], train1.columns[indices1[f]], importances1[indices1[f]]*100))\n",
    "\n",
    "\n",
    "# model.fit(train2, y_train2)\n",
    "# train_pred_rf2 = model.predict(train2)  #\n",
    "\n",
    "# y_pred_rf2 = model.predict(test2)  #\n",
    "\n",
    "# importances2 = model.feature_importances_\n",
    "# #std = np.std([tree.feature_importances_ for tree in model_rf.estimators_], axis=0)\n",
    "# indices2 = np.argsort(importances2)[::-1]\n",
    "\n",
    "# # Print the feature ranking\n",
    "# print(\"Feature ranking2:\")\n",
    "\n",
    "# for f in range(7):\n",
    "#     print(\"%d. feature %d.: %str (%2f)\" % (f + 1, indices2[f], train2.columns[indices2[f]], importances2[indices2[f]]*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mae_rf on train: 801.865598\n",
    "# Feature ranking:\n",
    "# 1. feature 166.: Premiumtr (29.192158)\n",
    "# 2. feature 167.: Premium_Predtr (26.174226)\n",
    "# 3. feature 258.: Premium_Pred_Addtr (8.405166)\n",
    "# 4. feature 72.: Replacement_cost_of_insured_vehicletr (5.817675)\n",
    "# 5. feature 71.: Engine_Displacement_(Cubic_Centimeter)tr (1.952259)\n",
    "# 6. feature 242.: Insured_Amount1_meantr (1.222075)\n",
    "# 7. feature 78.: aassured_ziptr (1.193084)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Final Training and Prediction: \n",
    "# stacked_averaged_models.fit(train.values, y_train.values)\n",
    "# stacked_train_pred = stacked_averaged_models.predict(train.values)\n",
    "# stacked_pred = stacked_averaged_models.predict(test.values)\n",
    "\n",
    "# print(mae_(y_train, stacked_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y_pred_lgb1))\n",
    "print(type(y_pred_xgb1))\n",
    "print(type(y_pred_lgb2))\n",
    "print(type(y_pred_xgb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = 1\n",
    "#x = 0.7\n",
    "#r = 1 - l - x\n",
    "# s = 1- l - x - r\n",
    "\n",
    "# ensemble_train = stacked_train_pred * s + train_pred_lgb *l + train_pred_xgb* x + train_pred_rf * r\n",
    "# ensemble_test = stacked_pred * s + y_pred_lgb * l + y_pred_xgb * x + y_pred_rf *r\n",
    "\n",
    "ensemble_train1 =  train_pred_lgb1 *l # + train_pred_xgb1* x \n",
    "ensemble_test1 =   y_pred_lgb1 * l # + y_ pred_xgb1 * x \n",
    "\n",
    "ensemble_train2 =  train_pred_lgb2 *l #+ train_pred_xgb2* x \n",
    "ensemble_test2 =   y_pred_lgb2 * l #+ y_pred_xgb2 * x \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE score on train data 1:\n",
      "0.173289120119\n",
      "MAE score on train data 2:\n",
      "0.145120352551\n"
     ]
    }
   ],
   "source": [
    "print('MAE score on train data 1:')  # 1844.5\n",
    "print(mae_1(y_train1, ensemble_train1))\n",
    "\n",
    "print('MAE score on train data 2:')  # 1844.5\n",
    "print(mae_2(y_train2, ensemble_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## prep submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final</th>\n",
       "      <th>lgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27953</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41825</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42198</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22467</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47607</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72034</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78517</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6743</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103978</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Final  lgb\n",
       "1990        1    1\n",
       "27953       1    1\n",
       "41825       1    1\n",
       "42198       1    1\n",
       "22467       1    1\n",
       "47607       1    1\n",
       "72034       1    1\n",
       "78517       1    1\n",
       "6743        1    1\n",
       "103978      1    1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred_stacked = pd.DataFrame(stacked_pred)\n",
    "y_pred_lgb1 = pd.DataFrame(y_pred_lgb1)\n",
    "# y_pred_xgb1 = pd.DataFrame(y_pred_xgb1)\n",
    "# y_pred_rf1 = pd.DataFrame(y_pred_rf1)\n",
    "\n",
    "y_pred1 = pd.DataFrame(ensemble_test1)\n",
    "\n",
    "all_y1 = pd.concat([y_pred1,y_pred_lgb1], axis=1)\n",
    "all_y1.columns=['Final', 'lgb']\n",
    "all_y1.to_csv('Cla_y1.csv')\n",
    "all_y1.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_stacked = pd.DataFrame(stacked_pred)\n",
    "y_pred_lgb2 = pd.DataFrame(y_pred_lgb2)\n",
    "y_pred_xgb2 = pd.DataFrame(y_pred_xgb2)\n",
    "# y_pred_rf2 = pd.DataFrame(y_pred_rf2)\n",
    "\n",
    "y_pred2 = pd.DataFrame(ensemble_test2)\n",
    "\n",
    "all_y2 = pd.concat([y_pred2,y_pred_lgb2, y_pred_xgb2], axis=1)\n",
    "all_y2.columns=['Final', 'lgb','xgb']\n",
    "all_y2.to_csv('Cla_y2.csv')\n",
    "all_y2.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Negative -> 0 \n",
    "b = all_y1[all_y1['Final']<0].shape[0]\n",
    "a = b/all_y1.shape[0]*100\n",
    "print((\"negative prediction1 (#): %2d\"%b))\n",
    "print(\"negative prediction1 (Percent): %2f\"%a)\n",
    "\n",
    "b = all_y2[all_y2['Final']<0].shape[0]\n",
    "a = b/all_y2.shape[0]*100\n",
    "print((\"negative prediction2 (#): %2d\"%b))\n",
    "print(\"negative prediction2 (Percent): %2f\"%a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# condition1 = all_y['Final']<0\n",
    "# all_y['Final'].loc[condition1] = 0\n",
    "# all_y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_testing.shape)\n",
    "print(test_ID1.shape)\n",
    "print(y_pred1.shape)\n",
    "print(test_ID2.shape)\n",
    "print(y_pred2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Policy_Number</th>\n",
       "      <th>Next_Premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55789b8f86893761c9aa9e7bf17938e737decc68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6df13a3384528ba6339c52b4fff7c149de68011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e112d926103147bcdcb6dab201b736185a3e2520</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa346fa4b1931d1c7a55f8e1bca40b0927dd65ac</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39c4d5daaa791676ec5559c9066d7e8e8dfc51d7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Policy_Number  Next_Premium\n",
       "0  55789b8f86893761c9aa9e7bf17938e737decc68             1\n",
       "1  b6df13a3384528ba6339c52b4fff7c149de68011             1\n",
       "2  e112d926103147bcdcb6dab201b736185a3e2520             0\n",
       "3  aa346fa4b1931d1c7a55f8e1bca40b0927dd65ac             1\n",
       "4  39c4d5daaa791676ec5559c9066d7e8e8dfc51d7             1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(test_ID1).reset_index()\n",
    "df1['Next_Premium'] = y_pred_lgb1\n",
    "\n",
    "df2 = pd.DataFrame(test_ID2).reset_index()\n",
    "df2['Next_Premium'] = y_pred_lgb2\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df = df_testing.merge(df, how = 'left', on = 'Policy_Number', suffixes=('_x','_y'))\n",
    "DF = df.drop(['index','Next_Premium_x'], axis = 1)\n",
    "DF.columns = df_testing.columns\n",
    "DF.to_csv('../Tbrain_Insurance/Classifier.csv',index = False)     ##\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import regression  df_y: \n",
    "\n",
    "df_y = pd.read_csv('../Tbrain_Insurance/testing-set_xgb.csv')\n",
    "print(df_y.shape)\n",
    "df_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df_reg & df_y\n",
    "\n",
    "df_final = df_cla.merge(df_y, on = 'Policy_Number')\n",
    "print(df_final.shape)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Next_Premium'] = df_final.apply(lambda row: row['Next_Premium_y'] if row['Next_Premium_x']==1 else 0, axis = 1 )\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(['Next_Premium_x', 'Next_Premium_y'], axis =1 )\n",
    "df_final.to_csv('testing-set.csv',index = False) \n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
